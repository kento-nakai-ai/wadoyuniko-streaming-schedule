# ç‰¹å…¸39: DevOpsãƒ»MLOpså®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰

## ğŸ¯ æ¦‚è¦
AIé§†å‹•é–‹ç™ºã«ãŠã‘ã‚‹DevOpsãƒ»MLOpsã‚¹ã‚­ãƒ«ã‚’ç¶²ç¾…çš„ã«ç¿’å¾—ã™ã‚‹ãŸã‚ã®å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ã€‚CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã‹ã‚‰ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥ã¾ã§ã€ç¾ä»£çš„ãªé–‹ç™ºãƒ»é‹ç”¨æ‰‹æ³•ã‚’ä½“ç³»çš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚

## ğŸ“‹ å­¦ç¿’ç›®æ¨™
- [ ] DevOpsæ–‡åŒ–ã¨åŸå‰‡ã‚’ç†è§£ã™ã‚‹
- [ ] CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] GitOpsæˆ¦ç•¥ã‚’å®Ÿè·µã™ã‚‹
- [ ] ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®è‡ªå‹•åŒ–ã‚’å®Ÿè£…ã™ã‚‹
- [ ] MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†ã‚’ç¿’å¾—ã™ã‚‹
- [ ] ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°æˆ¦ç•¥ã‚’å®Ÿè£…ã™ã‚‹
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨å“è³ªã‚’è‡ªå‹•åŒ–ã™ã‚‹

## ğŸš€ Phase 1: DevOpsåŸºç¤ã¨æ–‡åŒ–

### 1.1 DevOpsåŸå‰‡ã¨æ–‡åŒ–

```python
import json
import yaml
from datetime import datetime, timedelta
import subprocess
import os

class DevOpsPrinciples:
    """
    DevOpsåŸå‰‡ã¨å®Ÿè·µã®ç†è§£
    """
    
    def __init__(self):
        self.principles = {}
        self.practices = {}
        self.metrics = {}
    
    def devops_culture_fundamentals(self):
        """DevOpsæ–‡åŒ–ã®åŸºç¤"""
        culture_principles = {
            'ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': {
                'èª¬æ˜': 'é–‹ç™ºãƒãƒ¼ãƒ ã¨é‹ç”¨ãƒãƒ¼ãƒ ã®å£ã‚’å–ã‚Šé™¤ã',
                'å®Ÿè·µæ–¹æ³•': [
                    'ã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒŠãƒ«ãƒãƒ¼ãƒ ç·¨æˆ',
                    'å…±é€šãƒ„ãƒ¼ãƒ«ã¨ãƒ—ãƒ­ã‚»ã‚¹ã®ä½¿ç”¨',
                    'å®šæœŸçš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
                    'å…±æœ‰è²¬ä»»ã®æ–‡åŒ–é†¸æˆ'
                ]
            },
            'è‡ªå‹•åŒ–': {
                'èª¬æ˜': 'æ‰‹å‹•ä½œæ¥­ã‚’æœ€å°åŒ–ã—ã€ç¹°ã‚Šè¿”ã—ä½œæ¥­ã‚’è‡ªå‹•åŒ–',
                'å®Ÿè·µæ–¹æ³•': [
                    'CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰',
                    'ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®è‡ªå‹•åŒ–',
                    'ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–',
                    'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®è‡ªå‹•åŒ–'
                ]
            },
            'ç¶™ç¶šçš„æ”¹å–„': {
                'èª¬æ˜': 'å°ã•ãªå¤‰æ›´ã‚’ç¶™ç¶šçš„ã«è¡Œã„ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚’çŸ­ãã™ã‚‹',
                'å®Ÿè·µæ–¹æ³•': [
                    'é »ç¹ãªãƒªãƒªãƒ¼ã‚¹',
                    'A/Bãƒ†ã‚¹ãƒˆ',
                    'ãƒ¡ãƒˆãƒªã‚¯ã‚¹é§†å‹•ã®æ”¹å–„',
                    'ãƒã‚¹ãƒˆãƒ¢ãƒ¼ãƒ†ãƒ åˆ†æ'
                ]
            },
            'é¡§å®¢ä¸­å¿ƒ': {
                'èª¬æ˜': 'ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾¡å€¤å‰µå‡ºã‚’æœ€å„ªå…ˆ',
                'å®Ÿè·µæ–¹æ³•': [
                    'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†',
                    'ä¾¡å€¤ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°',
                    'ãƒªãƒ¼ãƒ³ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æ‰‹æ³•',
                    'ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼æœ€é©åŒ–'
                ]
            }
        }
        
        print("=== DevOpsæ–‡åŒ–ã®åŸå‰‡ ===")
        for principle, details in culture_principles.items():
            print(f"\n{principle}: {details['èª¬æ˜']}")
            print("å®Ÿè·µæ–¹æ³•:")
            for method in details['å®Ÿè·µæ–¹æ³•']:
                print(f"  â€¢ {method}")
        
        # DevOpsãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«
        lifecycle_stages = {
            'Plan': 'è¦ä»¶å®šç¾©ã€ã‚¹ãƒ—ãƒªãƒ³ãƒˆè¨ˆç”»ã€ãƒãƒƒã‚¯ãƒ­ã‚°ç®¡ç†',
            'Code': 'ã‚³ãƒ¼ãƒ‰é–‹ç™ºã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼',
            'Build': 'ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€ä¾å­˜é–¢ä¿‚è§£æ±ºã€æˆæœç‰©ä½œæˆ',
            'Test': 'è‡ªå‹•ãƒ†ã‚¹ãƒˆã€å“è³ªãƒã‚§ãƒƒã‚¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³',
            'Release': 'ãƒªãƒªãƒ¼ã‚¹è¨ˆç”»ã€æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ã€ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™',
            'Deploy': 'æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã€è¨­å®šç®¡ç†',
            'Operate': 'ã‚·ã‚¹ãƒ†ãƒ é‹ç”¨ã€ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†',
            'Monitor': 'ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€ãƒ­ã‚°åˆ†æã€ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†'
        }
        
        print("\n=== DevOpsãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ« ===")
        for stage, description in lifecycle_stages.items():
            print(f"{stage}: {description}")
        
        return culture_principles, lifecycle_stages
    
    def devops_metrics_and_kpis(self):
        """DevOpsãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨KPI"""
        metrics_categories = {
            'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé »åº¦': {
                'å®šç¾©': 'ã‚³ãƒ¼ãƒ‰ãŒæœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã‚‹é »åº¦',
                'ç›®æ¨™å€¤': '1æ—¥ã«è¤‡æ•°å›ï¼ˆã‚¨ãƒªãƒ¼ãƒˆ: ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ï¼‰',
                'æ¸¬å®šæ–¹æ³•': 'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ­ã‚°ã®åˆ†æ',
                'æ”¹å–„æ–½ç­–': ['CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–', 'è‡ªå‹•åŒ–ç‡å‘ä¸Š', 'ãƒ†ã‚¹ãƒˆåŠ¹ç‡åŒ–']
            },
            'ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ': {
                'å®šç¾©': 'ã‚³ãƒŸãƒƒãƒˆã‹ã‚‰æœ¬ç•ªåæ˜ ã¾ã§ã®æ™‚é–“',
                'ç›®æ¨™å€¤': '1æ™‚é–“æœªæº€ï¼ˆã‚¨ãƒªãƒ¼ãƒˆ: 1æ—¥æœªæº€ï¼‰',
                'æ¸¬å®šæ–¹æ³•': 'Gitå±¥æ­´ã¨ãƒ‡ãƒ—ãƒ­ã‚¤å±¥æ­´ã®ç›¸é–¢åˆ†æ',
                'æ”¹å–„æ–½ç­–': ['ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–', 'ãƒ†ã‚¹ãƒˆæœ€é©åŒ–', 'æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ç°¡ç´ åŒ–']
            },
            'å¹³å‡ä¿®å¾©æ™‚é–“ (MTTR)': {
                'å®šç¾©': 'ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç™ºç”Ÿã‹ã‚‰å¾©æ—§ã¾ã§ã®æ™‚é–“',
                'ç›®æ¨™å€¤': '1æ™‚é–“æœªæº€ï¼ˆã‚¨ãƒªãƒ¼ãƒˆ: 1æ™‚é–“æœªæº€ï¼‰',
                'æ¸¬å®šæ–¹æ³•': 'ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç®¡ç†ãƒ„ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿',
                'æ”¹å–„æ–½ç­–': ['ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è‡ªå‹•åŒ–', 'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–', 'ã‚ªãƒ³ã‚³ãƒ¼ãƒ«ä½“åˆ¶æ”¹å–„']
            },
            'å¤‰æ›´å¤±æ•—ç‡': {
                'å®šç¾©': 'ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚„ãƒ›ãƒƒãƒˆãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒå¿…è¦ãªå‰²åˆ',
                'ç›®æ¨™å€¤': '15%æœªæº€ï¼ˆã‚¨ãƒªãƒ¼ãƒˆ: 5%æœªæº€ï¼‰',
                'æ¸¬å®šæ–¹æ³•': 'å¤±æ•—ãƒ‡ãƒ—ãƒ­ã‚¤æ•° / ç·ãƒ‡ãƒ—ãƒ­ã‚¤æ•°',
                'æ”¹å–„æ–½ç­–': ['ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Š', 'ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹', 'ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤']
            }
        }
        
        print("=== DORA 4ã¤ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ===")
        for metric, details in metrics_categories.items():
            print(f"\n{metric}:")
            for key, value in details.items():
                if key == 'æ”¹å–„æ–½ç­–':
                    print(f"  {key}: {', '.join(value)}")
                else:
                    print(f"  {key}: {value}")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®å®Ÿè£…ä¾‹
        metrics_collection = '''
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®å®Ÿè£…ä¾‹
class DevOpsMetricsCollector:
    def __init__(self, git_repo, deploy_log, incident_log):
        self.git_repo = git_repo
        self.deploy_log = deploy_log
        self.incident_log = incident_log
    
    def calculate_deployment_frequency(self, period_days=30):
        """ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé »åº¦è¨ˆç®—"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=period_days)
        
        deployments = [
            d for d in self.deploy_log 
            if start_date <= d['timestamp'] <= end_date
        ]
        
        return len(deployments) / period_days
    
    def calculate_lead_time(self, commit_sha):
        """ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ è¨ˆç®—"""
        commit_time = self.get_commit_time(commit_sha)
        deploy_time = self.get_deploy_time(commit_sha)
        
        if commit_time and deploy_time:
            return (deploy_time - commit_time).total_seconds() / 3600  # æ™‚é–“å˜ä½
        return None
    
    def calculate_mttr(self, period_days=30):
        """å¹³å‡ä¿®å¾©æ™‚é–“è¨ˆç®—"""
        incidents = self.get_incidents_in_period(period_days)
        resolution_times = [
            (i['resolved_at'] - i['created_at']).total_seconds() / 3600
            for i in incidents if i['resolved_at']
        ]
        
        return sum(resolution_times) / len(resolution_times) if resolution_times else 0
'''
        
        print("\n=== ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Ÿè£…ä¾‹ ===")
        print(metrics_collection)
        
        return metrics_categories
    
    def create_devops_maturity_assessment(self):
        """DevOpsæˆç†Ÿåº¦è©•ä¾¡"""
        maturity_levels = {
            'ãƒ¬ãƒ™ãƒ«1: åˆæœŸ': {
                'ç‰¹å¾´': ['æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤', 'ã‚µã‚¤ãƒ­åŒ–ã•ã‚ŒãŸãƒãƒ¼ãƒ ', 'ä¸å®šæœŸãƒªãƒªãƒ¼ã‚¹'],
                'CI/CD': 'åŸºæœ¬çš„ãªãƒ“ãƒ«ãƒ‰è‡ªå‹•åŒ–',
                'ãƒ†ã‚¹ãƒˆ': 'æ‰‹å‹•ãƒ†ã‚¹ãƒˆä¸­å¿ƒ',
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': 'åŸºæœ¬çš„ãªãƒ­ã‚°ç¢ºèª',
                'æ–‡åŒ–': 'å¾“æ¥çš„ãªé–‹ç™ºãƒ»é‹ç”¨åˆ†é›¢'
            },
            'ãƒ¬ãƒ™ãƒ«2: ç®¡ç†': {
                'ç‰¹å¾´': ['åŸºæœ¬çš„ãªCI/CD', 'ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–é–‹å§‹', 'é€±æ¬¡ãƒªãƒªãƒ¼ã‚¹'],
                'CI/CD': 'ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤è‡ªå‹•åŒ–',
                'ãƒ†ã‚¹ãƒˆ': 'ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–',
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': 'APMãƒ„ãƒ¼ãƒ«å°å…¥',
                'æ–‡åŒ–': 'ãƒãƒ¼ãƒ é–“ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ”¹å–„'
            },
            'ãƒ¬ãƒ™ãƒ«3: å®šç¾©': {
                'ç‰¹å¾´': ['æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹', 'çµ±åˆãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–', 'æ—¥æ¬¡ãƒªãƒªãƒ¼ã‚¹'],
                'CI/CD': 'ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¨™æº–åŒ–',
                'ãƒ†ã‚¹ãƒˆ': 'E2Eãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–',
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': 'ãƒ¡ãƒˆãƒªã‚¯ã‚¹é§†å‹•ã®æ”¹å–„',
                'æ–‡åŒ–': 'DevOpsãƒãƒ¼ãƒ ç·¨æˆ'
            },
            'ãƒ¬ãƒ™ãƒ«4: å®šé‡ç®¡ç†': {
                'ç‰¹å¾´': ['ãƒ¡ãƒˆãƒªã‚¯ã‚¹é§†å‹•', 'ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹', 'æ™‚é–“å˜ä½ãƒªãƒªãƒ¼ã‚¹'],
                'CI/CD': 'ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤',
                'ãƒ†ã‚¹ãƒˆ': 'ãƒ†ã‚¹ãƒˆæˆ¦ç•¥æœ€é©åŒ–',
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': 'AIã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥',
                'æ–‡åŒ–': 'ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®æ„æ€æ±ºå®š'
            },
            'ãƒ¬ãƒ™ãƒ«5: æœ€é©åŒ–': {
                'ç‰¹å¾´': ['ç¶™ç¶šçš„æœ€é©åŒ–', 'ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°', 'ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ãƒªãƒªãƒ¼ã‚¹'],
                'CI/CD': 'GitOpså®Ÿè·µ',
                'ãƒ†ã‚¹ãƒˆ': 'ã‚·ãƒ•ãƒˆãƒ¬ãƒ•ãƒˆæˆ¦ç•¥',
                'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': 'äºˆæ¸¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°',
                'æ–‡åŒ–': 'å­¦ç¿’ã™ã‚‹çµ„ç¹”'
            }
        }
        
        print("=== DevOpsæˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ« ===")
        for level, characteristics in maturity_levels.items():
            print(f"\n{level}:")
            for aspect, description in characteristics.items():
                if aspect == 'ç‰¹å¾´':
                    print(f"  {aspect}: {', '.join(description)}")
                else:
                    print(f"  {aspect}: {description}")
        
        # æˆç†Ÿåº¦ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        checklist = {
            'ã‚³ãƒ¼ãƒ‰ç®¡ç†': [
                'ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ãŒãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚Œã¦ã„ã‚‹',
                'ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹',
                'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚‹',
                'ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹'
            ],
            'ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆ': [
                'ãƒ“ãƒ«ãƒ‰ãŒè‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚‹',
                'ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒè‡ªå‹•å®Ÿè¡Œã•ã‚Œã‚‹',
                'ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒæ¸¬å®šã•ã‚Œã¦ã„ã‚‹',
                'é™çš„è§£æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹'
            ],
            'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ': [
                'ãƒ‡ãƒ—ãƒ­ã‚¤ãŒè‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚‹',
                'è¤‡æ•°ç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå¯èƒ½',
                'ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ãŒã‚ã‚‹',
                'ãƒ‡ãƒ—ãƒ­ã‚¤æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚‹'
            ],
            'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': [
                'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†',
                'ã‚¤ãƒ³ãƒ•ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†',
                'ãƒ­ã‚°é›†ç´„ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚‹',
                'ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šãŒã‚ã‚‹'
            ]
        }
        
        print("\n=== æˆç†Ÿåº¦ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ===")
        for category, items in checklist.items():
            print(f"\n{category}:")
            for item in items:
                print(f"  â˜ {item}")
        
        return maturity_levels, checklist

# ä½¿ç”¨ä¾‹
devops_principles = DevOpsPrinciples()
culture, lifecycle = devops_principles.devops_culture_fundamentals()
metrics = devops_principles.devops_metrics_and_kpis()
maturity, checklist = devops_principles.create_devops_maturity_assessment()
```

### 1.2 CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

```python
class CICDPipeline:
    """
    CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã®å®Ÿè·µ
    """
    
    def __init__(self):
        self.pipeline_configs = {}
        self.tools = {}
    
    def cicd_pipeline_design(self):
        """CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ"""
        pipeline_stages = {
            'ã‚½ãƒ¼ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¸': {
                'ãƒˆãƒªã‚¬ãƒ¼': ['git push', 'pull request', 'scheduled'],
                'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³': ['ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰å–å¾—', 'ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯'],
                'ãƒ„ãƒ¼ãƒ«': ['GitHub', 'GitLab', 'Bitbucket'],
                'æˆæœç‰©': 'ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰'
            },
            'ãƒ“ãƒ«ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¸': {
                'ãƒˆãƒªã‚¬ãƒ¼': ['ã‚½ãƒ¼ã‚¹å¤‰æ›´æ¤œçŸ¥'],
                'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³': ['ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«', 'ä¾å­˜é–¢ä¿‚è§£æ±º', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä½œæˆ'],
                'ãƒ„ãƒ¼ãƒ«': ['Docker', 'Maven', 'npm', 'webpack'],
                'æˆæœç‰©': 'ãƒ“ãƒ«ãƒ‰ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ'
            },
            'ãƒ†ã‚¹ãƒˆã‚¹ãƒ†ãƒ¼ã‚¸': {
                'ãƒˆãƒªã‚¬ãƒ¼': ['ãƒ“ãƒ«ãƒ‰å®Œäº†'],
                'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³': ['ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ', 'çµ±åˆãƒ†ã‚¹ãƒˆ', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ'],
                'ãƒ„ãƒ¼ãƒ«': ['Jest', 'JUnit', 'SonarQube', 'OWASP ZAP'],
                'æˆæœç‰©': 'ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ'
            },
            'ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ãƒ†ãƒ¼ã‚¸': {
                'ãƒˆãƒªã‚¬ãƒ¼': ['ãƒ†ã‚¹ãƒˆæˆåŠŸ', 'æ‰‹å‹•æ‰¿èª'],
                'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³': ['ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤', 'è¨­å®šé©ç”¨', 'ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯'],
                'ãƒ„ãƒ¼ãƒ«': ['Kubernetes', 'Terraform', 'Ansible'],
                'æˆæœç‰©': 'ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³'
            }
        }
        
        print("=== CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ ===")
        for stage, details in pipeline_stages.items():
            print(f"\n{stage}:")
            for aspect, items in details.items():
                if isinstance(items, list):
                    print(f"  {aspect}: {', '.join(items)}")
                else:
                    print(f"  {aspect}: {items}")
        
        return pipeline_stages
    
    def create_github_actions_pipeline(self):
        """GitHub Actions CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        github_workflow = '''name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # SonarQubeã®ãŸã‚å…¨å±¥æ­´ã‚’å–å¾—

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Run linting
      run: npm run lint

    - name: Run type checking
      run: npm run type-check

    - name: SonarQube Scan
      uses: sonarqube-quality-gate-action@master
      env:
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [16, 18, 20]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run unit tests
      run: npm run test:unit -- --coverage
    
    - name: Run integration tests
      run: npm run test:integration
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³
  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run npm audit
      run: npm audit --audit-level moderate

  # Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
  build:
    needs: [code-quality, test, security]
    runs-on: ubuntu-latest
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  # é–‹ç™ºç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤
  deploy-dev:
    if: github.ref == 'refs/heads/develop'
    needs: build
    runs-on: ubuntu-latest
    environment: development
    
    steps:
    - name: Deploy to development
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/dev/deployment.yaml
          k8s/dev/service.yaml
        images: |
          ${{ needs.build.outputs.image-tag }}
        kubeconfig: ${{ secrets.KUBECONFIG_DEV }}

  # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤
  deploy-staging:
    if: github.ref == 'refs/heads/main'
    needs: build
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
    - name: Deploy to staging
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/staging/deployment.yaml
          k8s/staging/service.yaml
        images: |
          ${{ needs.build.outputs.image-tag }}
        kubeconfig: ${{ secrets.KUBECONFIG_STAGING }}
    
    - name: Run E2E tests
      run: |
        npm run test:e2e:staging
      env:
        STAGING_URL: https://staging.myapp.com

  # æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤
  deploy-prod:
    if: github.ref == 'refs/heads/main'
    needs: [build, deploy-staging]
    runs-on: ubuntu-latest
    environment: production
    
    steps:
    - name: Deploy to production
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/prod/deployment.yaml
          k8s/prod/service.yaml
        images: |
          ${{ needs.build.outputs.image-tag }}
        kubeconfig: ${{ secrets.KUBECONFIG_PROD }}
        strategy: canary
        percentage: 20
    
    - name: Verify deployment
      run: |
        kubectl rollout status deployment/myapp -n production
        npm run test:smoke:production
      env:
        PRODUCTION_URL: https://myapp.com
    
    - name: Complete canary deployment
      if: success()
      run: |
        kubectl patch deployment myapp -n production -p '{"spec":{"replicas":5}}'
    
    - name: Rollback on failure
      if: failure()
      run: |
        kubectl rollout undo deployment/myapp -n production

  # é€šçŸ¥
  notify:
    if: always()
    needs: [deploy-prod]
    runs-on: ubuntu-latest
    
    steps:
    - name: Slack notification
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
'''
        
        print("=== GitHub Actions CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
        print(github_workflow)
        
        return github_workflow
    
    def create_gitlab_cicd_pipeline(self):
        """GitLab CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        gitlab_cicd = '''# .gitlab-ci.yml
stages:
  - validate
  - build
  - test
  - security
  - deploy-dev
  - deploy-staging
  - deploy-prod

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  REGISTRY: registry.gitlab.com
  IMAGE_NAME: $CI_PROJECT_PATH
  KUBECONFIG: /tmp/kubeconfig

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®šç¾©
.docker: &docker
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind

.kubectl: &kubectl
  image: bitnami/kubectl:latest
  before_script:
    - echo "$KUBECONFIG_CONTENT" | base64 -d > $KUBECONFIG

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ®µéš
validate:code:
  stage: validate
  image: node:18
  cache:
    paths:
      - node_modules/
  script:
    - npm ci
    - npm run lint
    - npm run type-check
  artifacts:
    reports:
      junit: reports/lint-results.xml

# ãƒ“ãƒ«ãƒ‰æ®µéš
build:docker:
  <<: *docker
  stage: build
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - |
      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        tag="latest"
      else
        tag=$CI_COMMIT_REF_SLUG
      fi
    - docker build --pull -t "$CI_REGISTRY_IMAGE:$tag" .
    - docker push "$CI_REGISTRY_IMAGE:$tag"
  only:
    - main
    - develop

# ãƒ†ã‚¹ãƒˆæ®µéš
test:unit:
  stage: test
  image: node:18
  services:
    - postgres:15
  variables:
    POSTGRES_DB: test
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    DATABASE_URL: postgresql://postgres:postgres@postgres:5432/test
  cache:
    paths:
      - node_modules/
  script:
    - npm ci
    - npm run test:unit -- --coverage
    - npm run test:integration
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  artifacts:
    reports:
      junit: reports/test-results.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml

test:e2e:
  stage: test
  image: cypress/included:12.17.4
  script:
    - cypress run --record --key $CYPRESS_RECORD_KEY
  artifacts:
    when: always
    paths:
      - cypress/videos/
      - cypress/screenshots/
    expire_in: 1 week

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ®µéš
security:sast:
  stage: security
  variables:
    SAST_EXCLUDED_ANALYZERS: "semgrep"
  include:
    - template: Security/SAST.gitlab-ci.yml

security:dependency:
  stage: security
  include:
    - template: Security/Dependency-Scanning.gitlab-ci.yml

security:container:
  stage: security
  include:
    - template: Security/Container-Scanning.gitlab-ci.yml

# ãƒ‡ãƒ—ãƒ­ã‚¤æ®µéš
deploy:dev:
  <<: *kubectl
  stage: deploy-dev
  variables:
    KUBECONFIG_CONTENT: $KUBECONFIG_DEV
    NAMESPACE: development
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG -n $NAMESPACE
    - kubectl rollout status deployment/myapp -n $NAMESPACE
  environment:
    name: development
    url: https://dev.myapp.com
  only:
    - develop

deploy:staging:
  <<: *kubectl
  stage: deploy-staging
  variables:
    KUBECONFIG_CONTENT: $KUBECONFIG_STAGING
    NAMESPACE: staging
  script:
    - kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:latest -n $NAMESPACE
    - kubectl rollout status deployment/myapp -n $NAMESPACE
  environment:
    name: staging
    url: https://staging.myapp.com
  only:
    - main

deploy:prod:
  <<: *kubectl
  stage: deploy-prod
  variables:
    KUBECONFIG_CONTENT: $KUBECONFIG_PROD
    NAMESPACE: production
  script:
    - |
      # ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
      kubectl patch deployment myapp -n $NAMESPACE -p '{"spec":{"replicas":1}}'
      kubectl set image deployment/myapp myapp=$CI_REGISTRY_IMAGE:latest -n $NAMESPACE
      kubectl rollout status deployment/myapp -n $NAMESPACE
      
      # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
      sleep 60
      if curl -f https://myapp.com/health; then
        echo "Health check passed, scaling up"
        kubectl patch deployment myapp -n $NAMESPACE -p '{"spec":{"replicas":5}}'
      else
        echo "Health check failed, rolling back"
        kubectl rollout undo deployment/myapp -n $NAMESPACE
        exit 1
      fi
  environment:
    name: production
    url: https://myapp.com
  when: manual
  only:
    - main

# é€šçŸ¥
notify:success:
  stage: .post
  image: alpine:latest
  script:
    - apk add --no-cache curl
    - |
      curl -X POST -H 'Content-type: application/json' \
        --data "{\"text\":\"âœ… $CI_PROJECT_NAME deployed successfully to $CI_ENVIRONMENT_NAME\"}" \
        $SLACK_WEBHOOK_URL
  when: on_success
  only:
    - main

notify:failure:
  stage: .post
  image: alpine:latest
  script:
    - apk add --no-cache curl
    - |
      curl -X POST -H 'Content-type: application/json' \
        --data "{\"text\":\"âŒ $CI_PROJECT_NAME deployment failed in $CI_ENVIRONMENT_NAME\"}" \
        $SLACK_WEBHOOK_URL
  when: on_failure
  only:
    - main
'''
        
        print("=== GitLab CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
        print(gitlab_cicd)
        
        return gitlab_cicd
    
    def create_jenkins_pipeline(self):
        """Jenkins ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        jenkins_pipeline = '''pipeline {
    agent any
    
    environment {
        REGISTRY = 'docker.io'
        IMAGE_NAME = 'myapp'
        KUBECONFIG = credentials('kubeconfig')
        DOCKER_CREDENTIALS = credentials('docker-hub')
        SLACK_WEBHOOK = credentials('slack-webhook')
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    env.GIT_COMMIT_SHORT = sh(
                        script: "git rev-parse --short HEAD",
                        returnStdout: true
                    ).trim()
                }
            }
        }
        
        stage('Code Quality') {
            parallel {
                stage('Lint') {
                    steps {
                        sh 'npm ci'
                        sh 'npm run lint'
                    }
                    post {
                        always {
                            publishHTML([
                                allowMissing: false,
                                alwaysLinkToLastBuild: true,
                                keepAll: true,
                                reportDir: 'reports',
                                reportFiles: 'lint-results.html',
                                reportName: 'Lint Report'
                            ])
                        }
                    }
                }
                
                stage('SonarQube Analysis') {
                    steps {
                        withSonarQubeEnv('SonarQube') {
                            sh 'npm run sonar'
                        }
                    }
                }
            }
        }
        
        stage('Test') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        sh 'npm run test:unit -- --coverage'
                    }
                    post {
                        always {
                            publishTestResults testResultsPattern: 'reports/test-results.xml'
                            publishCoverage adapters: [
                                istanbulCoberturaAdapter('coverage/cobertura-coverage.xml')
                            ], sourceFileResolver: sourceFiles('STORE_LAST_BUILD')
                        }
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh '''
                            docker-compose -f docker-compose.test.yml up -d
                            npm run test:integration
                            docker-compose -f docker-compose.test.yml down
                        '''
                    }
                }
            }
        }
        
        stage('Security Scan') {
            parallel {
                stage('SAST') {
                    steps {
                        sh 'npm audit --audit-level moderate'
                        sh 'npx audit-ci --moderate'
                    }
                }
                
                stage('Container Scan') {
                    when {
                        anyOf {
                            branch 'main'
                            branch 'develop'
                        }
                    }
                    steps {
                        script {
                            sh "docker build -t ${IMAGE_NAME}:${GIT_COMMIT_SHORT} ."
                            sh "trivy image --exit-code 1 --severity HIGH,CRITICAL ${IMAGE_NAME}:${GIT_COMMIT_SHORT}"
                        }
                    }
                }
            }
        }
        
        stage('Build & Push') {
            when {
                anyOf {
                    branch 'main'
                    branch 'develop'
                }
            }
            steps {
                script {
                    def image = docker.build("${REGISTRY}/${IMAGE_NAME}:${GIT_COMMIT_SHORT}")
                    
                    docker.withRegistry("https://${REGISTRY}", 'docker-hub') {
                        image.push()
                        image.push("${BRANCH_NAME}-latest")
                        
                        if (env.BRANCH_NAME == 'main') {
                            image.push('latest')
                        }
                    }
                }
            }
        }
        
        stage('Deploy to Dev') {
            when {
                branch 'develop'
            }
            steps {
                script {
                    sh """
                        kubectl set image deployment/myapp myapp=${REGISTRY}/${IMAGE_NAME}:${GIT_COMMIT_SHORT} -n development
                        kubectl rollout status deployment/myapp -n development --timeout=300s
                    """
                }
            }
        }
        
        stage('Deploy to Staging') {
            when {
                branch 'main'
            }
            steps {
                script {
                    sh """
                        kubectl set image deployment/myapp myapp=${REGISTRY}/${IMAGE_NAME}:${GIT_COMMIT_SHORT} -n staging
                        kubectl rollout status deployment/myapp -n staging --timeout=300s
                    """
                }
            }
            post {
                success {
                    script {
                        sh 'npm run test:e2e -- --env staging'
                    }
                }
            }
        }
        
        stage('Approve Production Deploy') {
            when {
                branch 'main'
            }
            steps {
                script {
                    timeout(time: 5, unit: 'MINUTES') {
                        input message: 'Deploy to production?', ok: 'Deploy',
                              submitterParameter: 'APPROVER'
                    }
                }
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                script {
                    // ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
                    sh """
                        # ç¾åœ¨ã®ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’ä¿å­˜
                        CURRENT_REPLICAS=\$(kubectl get deployment myapp -n production -o jsonpath='{.spec.replicas}')
                        
                        # ã‚«ãƒŠãƒªã‚¢ç”¨ã«1ã¤ã®Podã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
                        kubectl patch deployment myapp -n production -p '{"spec":{"replicas":1}}'
                        kubectl set image deployment/myapp myapp=${REGISTRY}/${IMAGE_NAME}:${GIT_COMMIT_SHORT} -n production
                        kubectl rollout status deployment/myapp -n production --timeout=300s
                        
                        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
                        sleep 60
                        if curl -f https://myapp.com/health; then
                            echo "Health check passed, scaling up to original replica count"
                            kubectl patch deployment myapp -n production -p "{\"spec\":{\"replicas\":\$CURRENT_REPLICAS}}"
                            kubectl rollout status deployment/myapp -n production --timeout=600s
                        else
                            echo "Health check failed, rolling back"
                            kubectl rollout undo deployment/myapp -n production
                            exit 1
                        fi
                    """
                }
            }
        }
    }
    
    post {
        always {
            cleanWs()
        }
        
        success {
            script {
                if (env.BRANCH_NAME == 'main') {
                    sh """
                        curl -X POST -H 'Content-type: application/json' \
                            --data '{"text":"âœ… ${env.JOB_NAME} #${env.BUILD_NUMBER} deployed successfully to production by ${env.APPROVER}"}' \
                            ${SLACK_WEBHOOK}
                    """
                }
            }
        }
        
        failure {
            sh """
                curl -X POST -H 'Content-type: application/json' \
                    --data '{"text":"âŒ ${env.JOB_NAME} #${env.BUILD_NUMBER} failed"}' \
                    ${SLACK_WEBHOOK}
            """
        }
    }
}
'''
        
        print("=== Jenkins ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
        print(jenkins_pipeline)
        
        return jenkins_pipeline

# ä½¿ç”¨ä¾‹
cicd_pipeline = CICDPipeline()
pipeline_design = cicd_pipeline.cicd_pipeline_design()
github_workflow = cicd_pipeline.create_github_actions_pipeline()
gitlab_cicd = cicd_pipeline.create_gitlab_cicd_pipeline()
jenkins_pipeline = cicd_pipeline.create_jenkins_pipeline()
```

## ğŸ”„ Phase 2: GitOpsã¨ã‚¤ãƒ³ãƒ•ãƒ©è‡ªå‹•åŒ–

### 2.1 GitOpsæˆ¦ç•¥

```python
class GitOpsStrategy:
    """
    GitOpsæˆ¦ç•¥ã®å®Ÿè£…
    """
    
    def __init__(self):
        self.gitops_tools = {}
        self.workflows = {}
    
    def gitops_principles(self):
        """GitOpsåŸå‰‡"""
        principles = {
            'å®£è¨€çš„': {
                'èª¬æ˜': 'ã‚·ã‚¹ãƒ†ãƒ ã®æœ›ã¾ã—ã„çŠ¶æ…‹ã‚’ã‚³ãƒ¼ãƒ‰ã§å®£è¨€çš„ã«å®šç¾©',
                'å®Ÿè£…': [
                    'Kubernetesãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã§ãƒªã‚½ãƒ¼ã‚¹ã‚’YAMLã§å®šç¾©',
                    'Helmãƒãƒ£ãƒ¼ãƒˆã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ§‹æˆã‚’è¨˜è¿°',
                    'Terraformã§ã‚¤ãƒ³ãƒ•ãƒ©ã‚’å®£è¨€çš„ã«ç®¡ç†',
                    'ArgoCD Application CRDã§ç¶™ç¶šçš„ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’å®šç¾©'
                ]
            },
            'ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†': {
                'èª¬æ˜': 'ã™ã¹ã¦ã®è¨­å®šã¨çŠ¶æ…‹ãŒGitã§ç®¡ç†ã•ã‚Œã¦ã„ã‚‹',
                'å®Ÿè£…': [
                    'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’Gitãƒªãƒã‚¸ãƒˆãƒªã§ç®¡ç†',
                    'ã‚¤ãƒ³ãƒ•ãƒ©å®šç¾©ã‚’IaCã‚³ãƒ¼ãƒ‰ã¨ã—ã¦ä¿å­˜',
                    'ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥ã§ãƒªãƒªãƒ¼ã‚¹ç®¡ç†',
                    'ã‚¿ã‚°ã¨ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã§å¤‰æ›´å±¥æ­´ã‚’ç®¡ç†'
                ]
            },
            'è‡ªå‹•é©ç”¨': {
                'èª¬æ˜': 'Gitã®å¤‰æ›´ãŒè‡ªå‹•çš„ã«æœ¬ç•ªç’°å¢ƒã«åæ˜ ã•ã‚Œã‚‹',
                'å®Ÿè£…': [
                    'ArgoCD/Fluxã«ã‚ˆã‚‹ç¶™ç¶šçš„åŒæœŸ',
                    'Webhook/Polllingã«ã‚ˆã‚‹å¤‰æ›´æ¤œçŸ¥',
                    'Reconciliation Loopã§ã®çŠ¶æ…‹ç¢ºä¿',
                    'ã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½'
                ]
            },
            'ã‚»ã‚­ãƒ¥ã‚¢': {
                'èª¬æ˜': 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ç›£æŸ»å¯èƒ½æ€§ã‚’é‡è¦–',
                'å®Ÿè£…': [
                    'Pull-basedãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«',
                    'RBAC (Role-Based Access Control)',
                    'æš—å·åŒ–ã•ã‚ŒãŸã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†',
                    'å¤‰æ›´å±¥æ­´ã®å®Œå…¨ãªè¿½è·¡å¯èƒ½æ€§'
                ]
            }
        }
        
        print("=== GitOps 4ã¤ã®åŸå‰‡ ===")
        for principle, details in principles.items():
            print(f"\n{principle}: {details['èª¬æ˜']}")
            print("å®Ÿè£…æ–¹æ³•:")
            for implementation in details['å®Ÿè£…']:
                print(f"  â€¢ {implementation}")
        
        # GitOps vs å¾“æ¥æ‰‹æ³•ã®æ¯”è¼ƒ
        comparison = {
            'å¾“æ¥ã®CI/CD': {
                'ãƒ‡ãƒ—ãƒ­ã‚¤æ–¹å¼': 'Push-based (CI/CDãƒ„ãƒ¼ãƒ«ã‹ã‚‰ç’°å¢ƒã«ãƒ—ãƒƒã‚·ãƒ¥)',
                'ã‚¢ã‚¯ã‚»ã‚¹': 'CI/CDãƒ„ãƒ¼ãƒ«ãŒæœ¬ç•ªç’°å¢ƒã¸ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™å¿…è¦',
                'å¯è¦–æ€§': 'ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ç‚¹ã§ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ',
                'ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯': 'CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å†å®Ÿè¡ŒãŒå¿…è¦',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'CI/CDãƒ„ãƒ¼ãƒ«ãŒåºƒç¯„ãªæ¨©é™ã‚’æŒã¤'
            },
            'GitOps': {
                'ãƒ‡ãƒ—ãƒ­ã‚¤æ–¹å¼': 'Pull-based (ç’°å¢ƒå´ãŒGitã‹ã‚‰å¤‰æ›´ã‚’å–å¾—)',
                'ã‚¢ã‚¯ã‚»ã‚¹': 'GitOpsã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿ãŒå¿…è¦æœ€å°é™ã®æ¨©é™',
                'å¯è¦–æ€§': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®ç¾åœ¨çŠ¶æ…‹ã®å¯è¦–åŒ–',
                'ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯': 'Gitã§ã®ç°¡å˜ãªã‚³ãƒŸãƒƒãƒˆå±¥æ­´å¾©å…ƒ',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'æœ€å°æ¨©é™ã®åŸå‰‡ã€ç›£æŸ»ãƒ­ã‚°å®Œå‚™'
            }
        }
        
        print("\n=== GitOps vs å¾“æ¥æ‰‹æ³• ===")
        for aspect in comparison['å¾“æ¥ã®CI/CD'].keys():
            print(f"\n{aspect}:")
            print(f"  å¾“æ¥: {comparison['å¾“æ¥ã®CI/CD'][aspect]}")
            print(f"  GitOps: {comparison['GitOps'][aspect]}")
        
        return principles, comparison
    
    def create_argocd_application(self):
        """ArgoCD Applicationè¨­å®š"""
        argocd_app = '''apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-app
  namespace: argocd
  # Finalizerã‚’è¨­å®šã—ã¦ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚’é˜²ã
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
  project: default
  
  # ã‚½ãƒ¼ã‚¹è¨­å®šï¼ˆGitãƒªãƒã‚¸ãƒˆãƒªï¼‰
  source:
    repoURL: https://github.com/my-org/my-app-config
    targetRevision: HEAD
    path: manifests/production
    
    # Helmè¨­å®š
    helm:
      # values.yamlãƒ•ã‚¡ã‚¤ãƒ«
      valueFiles:
        - values.yaml
        - values-production.yaml
      
      # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
      parameters:
        - name: image.tag
          value: v1.2.3
        - name: replicaCount
          value: "5"
      
      # ãƒªãƒªãƒ¼ã‚¹å
      releaseName: my-app
  
  # ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆè¨­å®š
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  
  # åŒæœŸãƒãƒªã‚·ãƒ¼
  syncPolicy:
    # è‡ªå‹•åŒæœŸã‚’æœ‰åŠ¹åŒ–
    automated:
      # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆä¸è¦ãªãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤ï¼‰
      prune: true
      # ã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°
      selfHeal: true
      # åŒæœŸå‰ã«OutOfSyncãƒªã‚½ãƒ¼ã‚¹ã®ã¿å‰Šé™¤
      allowEmpty: false
    
    # åŒæœŸã‚ªãƒ—ã‚·ãƒ§ãƒ³
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
      - ApplyOutOfSyncOnly=true
      - RespectIgnoreDifferences=true
    
    # å†è©¦è¡Œè¨­å®š
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # ç„¡è¦–ã™ã‚‹å·®åˆ†
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas  # HPAãŒç®¡ç†ã™ã‚‹ãŸã‚ç„¡è¦–
    - group: ""
      kind: Secret
      name: my-app-secret
      jsonPointers:
        - /data  # å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ ãŒæ›´æ–°ã™ã‚‹ãŸã‚ç„¡è¦–
  
  # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯è¨­å®š
  revisionHistoryLimit: 10
---
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: my-project
  namespace: argocd
spec:
  description: My Application Project
  
  # ã‚½ãƒ¼ã‚¹ãƒªãƒã‚¸ãƒˆãƒªåˆ¶é™
  sourceRepos:
    - 'https://github.com/my-org/*'
    - 'https://charts.bitnami.com/bitnami'
  
  # ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆåˆ¶é™
  destinations:
    - namespace: 'production'
      server: https://kubernetes.default.svc
    - namespace: 'staging'
      server: https://kubernetes.default.svc
  
  # è¨±å¯ã•ã‚Œã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒªã‚½ãƒ¼ã‚¹
  clusterResourceWhitelist:
    - group: ''
      kind: Namespace
    - group: rbac.authorization.k8s.io
      kind: ClusterRole
    - group: rbac.authorization.k8s.io
      kind: ClusterRoleBinding
  
  # è¨±å¯ã•ã‚Œã‚‹ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ãƒªã‚½ãƒ¼ã‚¹
  namespaceResourceWhitelist:
    - group: ''
      kind: '*'
    - group: apps
      kind: '*'
    - group: networking.k8s.io
      kind: '*'
  
  # RBACè¨­å®š
  roles:
    - name: admin
      description: Full access to project applications
      policies:
        - p, proj:my-project:admin, applications, *, my-project/*, allow
        - p, proj:my-project:admin, repositories, *, *, allow
      groups:
        - my-org:team-leads
    
    - name: developer
      description: Limited access for developers
      policies:
        - p, proj:my-project:developer, applications, get, my-project/*, allow
        - p, proj:my-project:developer, applications, sync, my-project/*, allow
      groups:
        - my-org:developers
'''
        
        # ApplicationSetè¨­å®šï¼ˆè¤‡æ•°ç’°å¢ƒç®¡ç†ï¼‰
        appset_config = '''apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: my-app-environments
  namespace: argocd
spec:
  generators:
    # Git files generator - ç’°å¢ƒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹
    - git:
        repoURL: https://github.com/my-org/my-app-config
        revision: HEAD
        files:
          - path: "environments/*/config.json"
    
    # Cluster generator - è¤‡æ•°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¯¾å¿œ
    - clusters:
        selector:
          matchLabels:
            environment: production
        values:
          revision: main
    
    # Matrix generator - çµ„ã¿åˆã‚ã›
    - matrix:
        generators:
          - git:
              repoURL: https://github.com/my-org/my-app-config
              revision: HEAD
              directories:
                - path: applications/*
          - clusters:
              selector:
                matchLabels:
                  environment: staging
  
  template:
    metadata:
      name: '{{path.basename}}-{{name}}'
      labels:
        environment: '{{metadata.labels.environment}}'
    spec:
      project: my-project
      source:
        repoURL: https://github.com/my-org/my-app-config
        targetRevision: '{{revision}}'
        path: '{{path}}'
        helm:
          valueFiles:
            - 'values.yaml'
            - 'values-{{metadata.labels.environment}}.yaml'
          parameters:
            - name: environment
              value: '{{metadata.labels.environment}}'
            - name: cluster.name
              value: '{{name}}'
      destination:
        server: '{{server}}'
        namespace: '{{path.basename}}-{{metadata.labels.environment}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
'''
        
        print("=== ArgoCD Applicationè¨­å®š ===")
        print(argocd_app)
        print("\n=== ApplicationSetè¨­å®š ===")
        print(appset_config)
        
        return argocd_app, appset_config
    
    def create_flux_configuration(self):
        """Flux v2è¨­å®š"""
        flux_source = '''apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: GitRepository
metadata:
  name: my-app-source
  namespace: flux-system
spec:
  interval: 1m
  url: https://github.com/my-org/my-app-config
  ref:
    branch: main
  secretRef:
    name: git-credentials
---
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: bitnami
  namespace: flux-system
spec:
  interval: 10m
  url: https://charts.bitnami.com/bitnami
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: my-app
  namespace: production
spec:
  interval: 5m
  chart:
    spec:
      chart: ./charts/my-app
      sourceRef:
        kind: GitRepository
        name: my-app-source
        namespace: flux-system
      version: ">=1.0.0"
  
  # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
  values:
    image:
      repository: my-registry/my-app
      tag: v1.2.3
    replicaCount: 3
    
    ingress:
      enabled: true
      hosts:
        - host: myapp.production.com
          paths:
            - path: /
              pathType: Prefix
  
  # ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰è¨­å®š
  upgrade:
    remediation:
      retries: 3
    cleanupOnFail: true
  
  # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
  rollback:
    cleanupOnFail: true
    timeout: 10m
  
  # ãƒ†ã‚¹ãƒˆè¨­å®š
  test:
    enable: true
    timeout: 5m
  
  # ä¾å­˜é–¢ä¿‚
  dependsOn:
    - name: postgresql
      namespace: database
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
metadata:
  name: my-app-production
  namespace: flux-system
spec:
  interval: 10m
  sourceRef:
    kind: GitRepository
    name: my-app-source
  path: "./manifests/production"
  prune: true
  
  # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: my-app
      namespace: production
  
  # ä¾å­˜é–¢ä¿‚
  dependsOn:
    - name: infrastructure
  
  # å¾Œå‡¦ç†
  postBuild:
    substitute:
      cluster_name: "production-cluster"
      environment: "production"
    substituteFrom:
      - kind: ConfigMap
        name: cluster-config
      - kind: Secret
        name: cluster-secrets
'''
        
        # Flux ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
        flux_alerts = '''apiVersion: notification.toolkit.fluxcd.io/v1beta1
kind: Provider
metadata:
  name: slack
  namespace: flux-system
spec:
  type: slack
  channel: deployments
  secretRef:
    name: slack-webhook
---
apiVersion: notification.toolkit.fluxcd.io/v1beta1
kind: Alert
metadata:
  name: my-app-alerts
  namespace: flux-system
spec:
  providerRef:
    name: slack
  eventSeverity: info
  eventSources:
    - kind: GitRepository
      name: my-app-source
    - kind: Kustomization
      name: my-app-production
    - kind: HelmRelease
      name: my-app
      namespace: production
  summary: "Cluster {{.targetNamespace}}/{{.target}}"
  suspend: false
---
apiVersion: image.toolkit.fluxcd.io/v1beta1
kind: ImageRepository
metadata:
  name: my-app-image
  namespace: flux-system
spec:
  image: my-registry/my-app
  interval: 1m
---
apiVersion: image.toolkit.fluxcd.io/v1beta1
kind: ImagePolicy
metadata:
  name: my-app-policy
  namespace: flux-system
spec:
  imageRepositoryRef:
    name: my-app-image
  policy:
    semver:
      range: ">=1.0.0"
---
apiVersion: image.toolkit.fluxcd.io/v1beta1
kind: ImageUpdateAutomation
metadata:
  name: my-app-automation
  namespace: flux-system
spec:
  interval: 5m
  sourceRef:
    kind: GitRepository
    name: my-app-source
  git:
    checkout:
      ref:
        branch: main
    commit:
      author:
        email: flux@example.com
        name: Flux Bot
      messageTemplate: |
        Automated image update
        
        Automation name: {{ .AutomationObject }}
        
        Files:
        {{ range $filename, $_ := .Updated.Files -}}
        - {{ $filename }}
        {{ end -}}
        
        Objects:
        {{ range $resource, $_ := .Updated.Objects -}}
        - {{ $resource.Kind }} {{ $resource.Name }}
        {{ end -}}
        
        Images:
        {{ range .Updated.Images -}}
        - {{.}}
        {{ end -}}
    push:
      branch: main
  update:
    path: "./manifests"
    strategy: Setters
'''
        
        print("=== Flux v2 è¨­å®š ===")
        print(flux_source)
        print("\n=== Flux ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š ===")
        print(flux_alerts)
        
        return flux_source, flux_alerts

# ä½¿ç”¨ä¾‹
gitops = GitOpsStrategy()
principles, comparison = gitops.gitops_principles()
argocd_app, appset_config = gitops.create_argocd_application()
flux_source, flux_alerts = gitops.create_flux_configuration()
```

### 2.2 Infrastructure as Code ã®é«˜åº¦ãªå®Ÿè·µ

```python
class AdvancedIaC:
    """
    é«˜åº¦ãªInfrastructure as Codeå®Ÿè·µ
    """
    
    def __init__(self):
        self.iac_tools = {}
        self.best_practices = {}
    
    def iac_tool_comparison(self):
        """IaCãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ"""
        tools_comparison = {
            'Terraform': {
                'å­¦ç¿’ã‚³ã‚¹ãƒˆ': 'ä¸­',
                'å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€': 'éå¸¸ã«å¤šã„ï¼ˆ3000+ï¼‰',
                'çŠ¶æ…‹ç®¡ç†': 'ç‹¬è‡ªå½¢å¼ã®State file',
                'ãƒ—ãƒ©ãƒ³æ©Ÿèƒ½': 'å„ªç§€ï¼ˆterraform planï¼‰',
                'é©ç”¨å ´é¢': 'ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰ã€ã‚¤ãƒ³ãƒ•ãƒ©å…¨èˆ¬',
                'ç‰¹å¾´': ['HCLè¨€èª', 'ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ—ãƒ©ã‚°ã‚¤ãƒ³', 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–'],
                'ä¾‹': 'AWS, Azure, GCP, Kubernetesç­‰'
            },
            'Pulumi': {
                'å­¦ç¿’ã‚³ã‚¹ãƒˆ': 'ä¸­-é«˜',
                'å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€': 'å¤šã„ï¼ˆTerraformãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚‚åˆ©ç”¨å¯èƒ½ï¼‰',
                'çŠ¶æ…‹ç®¡ç†': 'Pulumi Serviceï¼ˆã¾ãŸã¯ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰',
                'ãƒ—ãƒ©ãƒ³æ©Ÿèƒ½': 'å„ªç§€ï¼ˆpulumi previewï¼‰',
                'é©ç”¨å ´é¢': 'è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯ã€æ—¢å­˜ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹çµ±åˆ',
                'ç‰¹å¾´': ['TypeScript/Python/Go/C#', 'å®Ÿãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª', 'ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆå¯èƒ½'],
                'ä¾‹': 'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã‚¤ãƒ³ãƒ•ãƒ©ã®çµ±åˆ'
            },
            'AWS CDK': {
                'å­¦ç¿’ã‚³ã‚¹ãƒˆ': 'ä¸­',
                'å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€': 'AWSå°‚ç”¨',
                'çŠ¶æ…‹ç®¡ç†': 'CloudFormation',
                'ãƒ—ãƒ©ãƒ³æ©Ÿèƒ½': 'å„ªç§€ï¼ˆcdk diffï¼‰',
                'é©ç”¨å ´é¢': 'AWSç‰¹åŒ–ã€å‹å®‰å…¨æ€§é‡è¦–',
                'ç‰¹å¾´': ['TypeScript/Python/Javaç­‰', 'ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ', 'é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡åŒ–'],
                'ä¾‹': 'AWS Lambda + API Gateway + DynamoDB'
            },
            'Ansible': {
                'å­¦ç¿’ã‚³ã‚¹ãƒˆ': 'ä½-ä¸­',
                'å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€': 'å¤šã„ï¼ˆè¨­å®šç®¡ç†ä¸­å¿ƒï¼‰',
                'çŠ¶æ…‹ç®¡ç†': 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ã‚¹',
                'ãƒ—ãƒ©ãƒ³æ©Ÿèƒ½': 'åŸºæœ¬çš„ï¼ˆ--check ãƒ¢ãƒ¼ãƒ‰ï¼‰',
                'é©ç”¨å ´é¢': 'è¨­å®šç®¡ç†ã€ã‚µãƒ¼ãƒãƒ¼æ§‹æˆ',
                'ç‰¹å¾´': ['YAML', 'ãƒ—ãƒ¬ã‚¤ãƒ–ãƒƒã‚¯', 'ã¹ãç­‰æ€§'],
                'ä¾‹': 'ã‚µãƒ¼ãƒãƒ¼è¨­å®šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤'
            }
        }
        
        print("=== IaCãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ ===")
        for tool, features in tools_comparison.items():
            print(f"\n{tool}:")
            for feature, value in features.items():
                if feature == 'ç‰¹å¾´':
                    print(f"  {feature}: {', '.join(value)}")
                else:
                    print(f"  {feature}: {value}")
        
        # é¸æŠåŸºæº–
        selection_criteria = {
            'ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œ': 'Terraform, Pulumi',
            'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªçµ±åˆ': 'Pulumi, AWS CDK',
            'AWSç‰¹åŒ–': 'AWS CDK, Terraform',
            'å­¦ç¿’ã‚³ã‚¹ãƒˆé‡è¦–': 'Ansible, Terraform',
            'ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆ': 'Terraform, Ansible',
            'ä¼æ¥­ã‚µãƒãƒ¼ãƒˆ': 'ã™ã¹ã¦ï¼ˆå•†ç”¨ç‰ˆã‚ã‚Šï¼‰'
        }
        
        print("\n=== é¸æŠåŸºæº– ===")
        for criteria, recommendation in selection_criteria.items():
            print(f"{criteria}: {recommendation}")
        
        return tools_comparison
    
    def create_pulumi_infrastructure(self):
        """Pulumi ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ä¾‹"""
        pulumi_code = '''import pulumi
import pulumi_aws as aws
import pulumi_kubernetes as k8s
from typing import List, Dict, Any

class InfrastructureStack:
    """AWS + Kubernetes ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚¹ã‚¿ãƒƒã‚¯"""
    
    def __init__(self):
        self.config = pulumi.Config()
        self.project_name = pulumi.get_project()
        self.stack_name = pulumi.get_stack()
        
        # å…±é€šã‚¿ã‚°
        self.common_tags = {
            "Project": self.project_name,
            "Environment": self.stack_name,
            "ManagedBy": "pulumi"
        }
        
        # ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰
        self.vpc = self._create_vpc()
        self.eks_cluster = self._create_eks_cluster()
        self.rds_instance = self._create_rds_instance()
        self.redis_cluster = self._create_redis_cluster()
        
        # Kubernetesãƒªã‚½ãƒ¼ã‚¹
        self.k8s_provider = self._create_k8s_provider()
        self.app_namespace = self._create_app_namespace()
        self.app_deployment = self._create_app_deployment()
        
        # å‡ºåŠ›å€¤
        self._export_outputs()
    
    def _create_vpc(self) -> aws.ec2.Vpc:
        """VPCä½œæˆ"""
        vpc = aws.ec2.Vpc(
            f"{self.project_name}-vpc",
            cidr_block="10.0.0.0/16",
            enable_dns_hostnames=True,
            enable_dns_support=True,
            tags={**self.common_tags, "Name": f"{self.project_name}-vpc"}
        )
        
        # ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚µãƒ–ãƒãƒƒãƒˆ
        public_subnets = []
        for i, az in enumerate(["us-west-2a", "us-west-2b"]):
            subnet = aws.ec2.Subnet(
                f"{self.project_name}-public-subnet-{i+1}",
                vpc_id=vpc.id,
                cidr_block=f"10.0.{i+1}.0/24",
                availability_zone=az,
                map_public_ip_on_launch=True,
                tags={**self.common_tags, "Name": f"{self.project_name}-public-{i+1}"}
            )
            public_subnets.append(subnet)
        
        # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚µãƒ–ãƒãƒƒãƒˆ
        private_subnets = []
        for i, az in enumerate(["us-west-2a", "us-west-2b"]):
            subnet = aws.ec2.Subnet(
                f"{self.project_name}-private-subnet-{i+1}",
                vpc_id=vpc.id,
                cidr_block=f"10.0.{i+10}.0/24",
                availability_zone=az,
                tags={**self.common_tags, "Name": f"{self.project_name}-private-{i+1}"}
            )
            private_subnets.append(subnet)
        
        # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤
        igw = aws.ec2.InternetGateway(
            f"{self.project_name}-igw",
            vpc_id=vpc.id,
            tags={**self.common_tags, "Name": f"{self.project_name}-igw"}
        )
        
        # NATã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤
        for i, subnet in enumerate(public_subnets):
            eip = aws.ec2.Eip(
                f"{self.project_name}-nat-eip-{i+1}",
                domain="vpc",
                tags={**self.common_tags, "Name": f"{self.project_name}-nat-eip-{i+1}"}
            )
            
            nat_gw = aws.ec2.NatGateway(
                f"{self.project_name}-nat-{i+1}",
                allocation_id=eip.id,
                subnet_id=subnet.id,
                tags={**self.common_tags, "Name": f"{self.project_name}-nat-{i+1}"}
            )
        
        # è¿½åŠ ã®VPCè¨­å®šã‚’ã“ã“ã«...
        self.public_subnets = public_subnets
        self.private_subnets = private_subnets
        
        return vpc
    
    def _create_eks_cluster(self) -> aws.eks.Cluster:
        """EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ"""
        # IAMãƒ­ãƒ¼ãƒ«
        cluster_role = aws.iam.Role(
            f"{self.project_name}-eks-cluster-role",
            assume_role_policy="""{
                "Version": "2012-10-17",
                "Statement": [{
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Principal": {"Service": "eks.amazonaws.com"}
                }]
            }""",
            tags=self.common_tags
        )
        
        aws.iam.RolePolicyAttachment(
            f"{self.project_name}-eks-cluster-policy",
            role=cluster_role.name,
            policy_arn="arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
        )
        
        # EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        cluster = aws.eks.Cluster(
            f"{self.project_name}-cluster",
            role_arn=cluster_role.arn,
            version="1.27",
            vpc_config=aws.eks.ClusterVpcConfigArgs(
                subnet_ids=[s.id for s in self.public_subnets + self.private_subnets],
                endpoint_private_access=True,
                endpoint_public_access=True,
                public_access_cidrs=["0.0.0.0/0"]
            ),
            enabled_cluster_log_types=["api", "audit", "authenticator", "controllerManager", "scheduler"],
            tags=self.common_tags
        )
        
        # ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—
        node_role = aws.iam.Role(
            f"{self.project_name}-eks-node-role",
            assume_role_policy="""{
                "Version": "2012-10-17",
                "Statement": [{
                    "Action": "sts:AssumeRole",
                    "Effect": "Allow",
                    "Principal": {"Service": "ec2.amazonaws.com"}
                }]
            }""",
            tags=self.common_tags
        )
        
        for policy in [
            "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
            "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
            "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
        ]:
            aws.iam.RolePolicyAttachment(
                f"{self.project_name}-node-policy-{policy.split('/')[-1]}",
                role=node_role.name,
                policy_arn=policy
            )
        
        node_group = aws.eks.NodeGroup(
            f"{self.project_name}-node-group",
            cluster_name=cluster.name,
            node_role_arn=node_role.arn,
            subnet_ids=[s.id for s in self.private_subnets],
            instance_types=["t3.medium"],
            scaling_config=aws.eks.NodeGroupScalingConfigArgs(
                desired_size=3,
                max_size=10,
                min_size=1
            ),
            update_config=aws.eks.NodeGroupUpdateConfigArgs(
                max_unavailable=1
            ),
            tags=self.common_tags
        )
        
        return cluster
    
    def _create_rds_instance(self) -> aws.rds.Instance:
        """RDSã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ"""
        # ã‚µãƒ–ãƒãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—
        db_subnet_group = aws.rds.SubnetGroup(
            f"{self.project_name}-db-subnet-group",
            subnet_ids=[s.id for s in self.private_subnets],
            tags=self.common_tags
        )
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—
        db_security_group = aws.ec2.SecurityGroup(
            f"{self.project_name}-db-sg",
            vpc_id=self.vpc.id,
            ingress=[aws.ec2.SecurityGroupIngressArgs(
                from_port=5432,
                to_port=5432,
                protocol="tcp",
                cidr_blocks=["10.0.0.0/16"]
            )],
            tags=self.common_tags
        )
        
        # RDSã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        return aws.rds.Instance(
            f"{self.project_name}-db",
            engine="postgres",
            engine_version="15.3",
            instance_class="db.t3.micro",
            allocated_storage=20,
            storage_type="gp2",
            storage_encrypted=True,
            db_name="myapp",
            username="postgres",
            password=self.config.require_secret("db_password"),
            vpc_security_group_ids=[db_security_group.id],
            db_subnet_group_name=db_subnet_group.name,
            backup_retention_period=7,
            backup_window="03:00-04:00",
            maintenance_window="Sun:04:00-Sun:05:00",
            skip_final_snapshot=True,
            tags=self.common_tags
        )
    
    def _create_redis_cluster(self) -> aws.elasticache.ReplicationGroup:
        """Redis ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ"""
        cache_subnet_group = aws.elasticache.SubnetGroup(
            f"{self.project_name}-cache-subnet-group",
            subnet_ids=[s.id for s in self.private_subnets]
        )
        
        cache_security_group = aws.ec2.SecurityGroup(
            f"{self.project_name}-cache-sg",
            vpc_id=self.vpc.id,
            ingress=[aws.ec2.SecurityGroupIngressArgs(
                from_port=6379,
                to_port=6379,
                protocol="tcp",
                cidr_blocks=["10.0.0.0/16"]
            )],
            tags=self.common_tags
        )
        
        return aws.elasticache.ReplicationGroup(
            f"{self.project_name}-cache",
            description="Redis cluster for my app",
            node_type="cache.t3.micro",
            port=6379,
            parameter_group_name="default.redis7",
            num_cache_clusters=2,
            subnet_group_name=cache_subnet_group.name,
            security_group_ids=[cache_security_group.id],
            at_rest_encryption_enabled=True,
            transit_encryption_enabled=True,
            tags=self.common_tags
        )
    
    def _create_k8s_provider(self) -> k8s.Provider:
        """Kubernetesãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆ"""
        return k8s.Provider(
            "k8s-provider",
            kubeconfig=pulumi.Output.all(
                self.eks_cluster.endpoint,
                self.eks_cluster.certificate_authority.apply(lambda ca: ca.data),
                self.eks_cluster.name
            ).apply(lambda args: self._generate_kubeconfig(*args))
        )
    
    def _generate_kubeconfig(self, endpoint: str, cert_data: str, cluster_name: str) -> str:
        """kubeconfigç”Ÿæˆ"""
        return f"""
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: {cert_data}
    server: {endpoint}
  name: {cluster_name}
contexts:
- context:
    cluster: {cluster_name}
    user: {cluster_name}
  name: {cluster_name}
current-context: {cluster_name}
kind: Config
preferences: {{}}
users:
- name: {cluster_name}
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: aws
      args:
        - eks
        - get-token
        - --cluster-name
        - {cluster_name}
"""
    
    def _create_app_namespace(self) -> k8s.core.v1.Namespace:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ"""
        return k8s.core.v1.Namespace(
            f"{self.project_name}-namespace",
            metadata=k8s.meta.v1.ObjectMetaArgs(
                name=self.project_name,
                labels={"app": self.project_name}
            ),
            opts=pulumi.ResourceOptions(provider=self.k8s_provider)
        )
    
    def _create_app_deployment(self) -> k8s.apps.v1.Deployment:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆä½œæˆ"""
        return k8s.apps.v1.Deployment(
            f"{self.project_name}-deployment",
            metadata=k8s.meta.v1.ObjectMetaArgs(
                namespace=self.app_namespace.metadata.name,
                name=self.project_name
            ),
            spec=k8s.apps.v1.DeploymentSpecArgs(
                replicas=3,
                selector=k8s.meta.v1.LabelSelectorArgs(
                    match_labels={"app": self.project_name}
                ),
                template=k8s.core.v1.PodTemplateSpecArgs(
                    metadata=k8s.meta.v1.ObjectMetaArgs(
                        labels={"app": self.project_name}
                    ),
                    spec=k8s.core.v1.PodSpecArgs(
                        containers=[k8s.core.v1.ContainerArgs(
                            name=self.project_name,
                            image=f"nginx:1.21",
                            ports=[k8s.core.v1.ContainerPortArgs(
                                container_port=80
                            )],
                            env=[
                                k8s.core.v1.EnvVarArgs(
                                    name="DATABASE_URL",
                                    value=pulumi.Output.concat(
                                        "postgresql://postgres:",
                                        self.config.require_secret("db_password"),
                                        "@",
                                        self.rds_instance.endpoint,
                                        ":5432/myapp"
                                    )
                                ),
                                k8s.core.v1.EnvVarArgs(
                                    name="REDIS_URL",
                                    value=pulumi.Output.concat(
                                        "redis://",
                                        self.redis_cluster.configuration_endpoint_address,
                                        ":6379"
                                    )
                                )
                            ]
                        )]
                    )
                )
            ),
            opts=pulumi.ResourceOptions(provider=self.k8s_provider)
        )
    
    def _export_outputs(self):
        """å‡ºåŠ›å€¤è¨­å®š"""
        pulumi.export("vpc_id", self.vpc.id)
        pulumi.export("cluster_name", self.eks_cluster.name)
        pulumi.export("cluster_endpoint", self.eks_cluster.endpoint)
        pulumi.export("rds_endpoint", self.rds_instance.endpoint)
        pulumi.export("redis_endpoint", self.redis_cluster.configuration_endpoint_address)

# ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆ
stack = InfrastructureStack()
'''
        
        print("=== Pulumi ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å®Ÿè£…ä¾‹ ===")
        print(pulumi_code)
        
        return pulumi_code
    
    def create_testing_strategy(self):
        """IaCãƒ†ã‚¹ãƒˆæˆ¦ç•¥"""
        testing_approaches = {
            'Static Analysis': {
                'ç›®çš„': 'ã‚³ãƒ¼ãƒ‰å“è³ªã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯',
                'ãƒ„ãƒ¼ãƒ«': ['tflint', 'checkov', 'terrascan', 'tfsec'],
                'æ¤œè¨¼é …ç›®': [
                    'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹éµå®ˆ',
                    'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š',
                    'ãƒªã‚½ãƒ¼ã‚¹å‘½åè¦å‰‡',
                    'å»ƒæ­¢äºˆå®šæ©Ÿèƒ½ã®ä½¿ç”¨'
                ]
            },
            'Unit Testing': {
                'ç›®çš„': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½“ã®å‹•ä½œç¢ºèª',
                'ãƒ„ãƒ¼ãƒ«': ['terratest', 'pytest', 'go test'],
                'æ¤œè¨¼é …ç›®': [
                    'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…¥å‡ºåŠ›',
                    'ãƒªã‚½ãƒ¼ã‚¹å±æ€§',
                    'æ¡ä»¶åˆ†å²ãƒ­ã‚¸ãƒƒã‚¯',
                    'ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°'
                ]
            },
            'Integration Testing': {
                'ç›®çš„': 'å®Ÿç’°å¢ƒã§ã®å‹•ä½œç¢ºèª',
                'ãƒ„ãƒ¼ãƒ«': ['terratest', 'kitchen-terraform', 'inspec'],
                'æ¤œè¨¼é …ç›®': [
                    'ãƒªã‚½ãƒ¼ã‚¹é–“ã®é€£æº',
                    'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šæ€§',
                    'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šåŠ¹æœ',
                    'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹'
                ]
            },
            'End-to-End Testing': {
                'ç›®çš„': 'ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å‹•ä½œç¢ºèª',
                'ãƒ„ãƒ¼ãƒ«': ['terratest', 'cypress', 'postman'],
                'æ¤œè¨¼é …ç›®': [
                    'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œ',
                    'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒŠãƒªã‚ª',
                    'ç½å®³å¾©æ—§',
                    'ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'
                ]
            }
        }
        
        print("=== IaCãƒ†ã‚¹ãƒˆæˆ¦ç•¥ ===")
        for approach, details in testing_approaches.items():
            print(f"\n{approach}: {details['ç›®çš„']}")
            print(f"ãƒ„ãƒ¼ãƒ«: {', '.join(details['ãƒ„ãƒ¼ãƒ«'])}")
            print("æ¤œè¨¼é …ç›®:")
            for item in details['æ¤œè¨¼é …ç›®']:
                print(f"  â€¢ {item}")
        
        # Terratestå®Ÿè£…ä¾‹
        terratest_example = '''package test

import (
    "testing"
    "time"

    "github.com/gruntwork-io/terratest/modules/aws"
    "github.com/gruntwork-io/terratest/modules/terraform"
    "github.com/stretchr/testify/assert"
)

func TestTerraformEKSCluster(t *testing.T) {
    t.Parallel()

    // Terraformè¨­å®š
    terraformOptions := terraform.WithDefaultRetryableErrors(t, &terraform.Options{
        TerraformDir: "../examples/eks",
        Vars: map[string]interface{}{
            "cluster_name": "test-cluster",
            "region":       "us-west-2",
        },
    })

    // ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤
    defer terraform.Destroy(t, terraformOptions)

    // Terraformã§ã‚¤ãƒ³ãƒ•ãƒ©ä½œæˆ
    terraform.InitAndApply(t, terraformOptions)

    // å‡ºåŠ›å€¤å–å¾—
    clusterName := terraform.Output(t, terraformOptions, "cluster_name")
    clusterEndpoint := terraform.Output(t, terraformOptions, "cluster_endpoint")
    vpcId := terraform.Output(t, terraformOptions, "vpc_id")

    // ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
    assert.Equal(t, "test-cluster", clusterName)
    assert.Contains(t, clusterEndpoint, "eks.us-west-2.amazonaws.com")

    // AWSãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª
    region := "us-west-2"
    
    // EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å­˜åœ¨ç¢ºèª
    cluster := aws.GetEksCluster(t, region, clusterName)
    assert.Equal(t, "ACTIVE", *cluster.Status)
    
    // VPCç¢ºèª
    vpc := aws.GetVpcById(t, vpcId, region)
    assert.Equal(t, "10.0.0.0/16", *vpc.CidrBlock)

    // ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ç¢ºèª
    nodeGroups := aws.GetEksClusterNodeGroups(t, region, clusterName)
    assert.Greater(t, len(nodeGroups), 0)

    // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ç¢ºèª
    securityGroups := aws.GetSecurityGroupsForVpc(t, vpcId, region)
    assert.Greater(t, len(securityGroups), 0)

    // ã‚¿ã‚°ç¢ºèª
    expectedTags := map[string]string{
        "Environment": "test",
        "ManagedBy":   "terraform",
    }
    aws.AssertEc2TagsExist(t, region, vpcId, expectedTags)
}

func TestTerraformVPCNetworking(t *testing.T) {
    t.Parallel()

    terraformOptions := &terraform.Options{
        TerraformDir: "../modules/vpc",
        Vars: map[string]interface{}{
            "vpc_cidr": "10.1.0.0/16",
            "azs":      []string{"us-west-2a", "us-west-2b"},
        },
    }

    defer terraform.Destroy(t, terraformOptions)
    terraform.InitAndApply(t, terraformOptions)

    // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆ
    vpcId := terraform.Output(t, terraformOptions, "vpc_id")
    publicSubnetIds := terraform.OutputList(t, terraformOptions, "public_subnet_ids")
    privateSubnetIds := terraform.OutputList(t, terraformOptions, "private_subnet_ids")

    region := "us-west-2"

    // ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚µãƒ–ãƒãƒƒãƒˆã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèª
    for _, subnetId := range publicSubnetIds {
        subnet := aws.GetSubnetById(t, subnetId, region)
        assert.True(t, *subnet.MapPublicIpOnLaunch)
    }

    // ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚µãƒ–ãƒãƒƒãƒˆã®NATæ¥ç¶šç¢ºèª
    for _, subnetId := range privateSubnetIds {
        subnet := aws.GetSubnetById(t, subnetId, region)
        assert.False(t, *subnet.MapPublicIpOnLaunch)
    }

    // ãƒ«ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
    routeTables := aws.GetRouteTablesForVpc(t, vpcId, region)
    assert.Greater(t, len(routeTables), 0)
}
'''
        
        print("\n=== Terratestå®Ÿè£…ä¾‹ ===")
        print(terratest_example)
        
        return testing_approaches, terratest_example

# ä½¿ç”¨ä¾‹
advanced_iac = AdvancedIaC()
tools_comparison = advanced_iac.iac_tool_comparison()
pulumi_code = advanced_iac.create_pulumi_infrastructure()
testing_approaches, terratest_example = advanced_iac.create_testing_strategy()
```

## ğŸ¤– Phase 3: MLOpså®Ÿè·µ

### 3.1 æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
class MLOpsPipeline:
    """
    MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…
    """
    
    def __init__(self):
        self.pipeline_components = {}
        self.ml_tools = {}
    
    def mlops_lifecycle(self):
        """MLOpsãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«"""
        lifecycle_stages = {
            'ãƒ‡ãƒ¼ã‚¿ç®¡ç†': {
                'ç›®çš„': 'ãƒ‡ãƒ¼ã‚¿ã®åé›†ã€å‰å‡¦ç†ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†',
                'ãƒ„ãƒ¼ãƒ«': ['DVC', 'MLflow', 'Feast', 'Great Expectations'],
                'ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£': [
                    'ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å–å¾—',
                    'ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»å“è³ªãƒã‚§ãƒƒã‚¯',
                    'ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°',
                    'ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°',
                    'ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ç®¡ç†'
                ]
            },
            'ãƒ¢ãƒ‡ãƒ«é–‹ç™º': {
                'ç›®çš„': 'ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã€å®Ÿé¨“ã€è©•ä¾¡',
                'ãƒ„ãƒ¼ãƒ«': ['Jupyter', 'MLflow', 'Weights & Biases', 'TensorBoard'],
                'ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£': [
                    'å®Ÿé¨“ç®¡ç†ãƒ»è¿½è·¡',
                    'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°',
                    'ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ»æ¯”è¼ƒ',
                    'ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°',
                    'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç®¡ç†'
                ]
            },
            'ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼': {
                'ç›®çš„': 'ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª',
                'ãƒ„ãƒ¼ãƒ«': ['Great Expectations', 'MLflow', 'Evidently', 'Alibi'],
                'ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£': [
                    'ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ',
                    'ãƒã‚¤ã‚¢ã‚¹ãƒ»å…¬å¹³æ€§ãƒã‚§ãƒƒã‚¯',
                    'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼',
                    'A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ',
                    'æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹'
                ]
            },
            'ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤': {
                'ç›®çš„': 'ãƒ¢ãƒ‡ãƒ«ã®æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹',
                'ãƒ„ãƒ¼ãƒ«': ['Kubernetes', 'Docker', 'Seldon', 'KServe', 'BentoML'],
                'ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£': [
                    'ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°',
                    'ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤',
                    'ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤',
                    'ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²',
                    'ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½'
                ]
            },
            'ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°': {
                'ç›®çš„': 'ãƒ¢ãƒ‡ãƒ«ã®ç¶™ç¶šçš„ãªç›£è¦–ã¨æ”¹å–„',
                'ãƒ„ãƒ¼ãƒ«': ['Prometheus', 'Grafana', 'Evidently', 'Fiddler'],
                'ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£': [
                    'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–',
                    'ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º',
                    'ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º',
                    'ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†',
                    'å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼'
                ]
            }
        }
        
        print("=== MLOpsãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ« ===")
        for stage, details in lifecycle_stages.items():
            print(f"\n{stage}: {details['ç›®çš„']}")
            print(f"ãƒ„ãƒ¼ãƒ«: {', '.join(details['ãƒ„ãƒ¼ãƒ«'])}")
            print("ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£:")
            for activity in details['ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£']:
                print(f"  â€¢ {activity}")
        
        return lifecycle_stages
    
    def create_kubeflow_pipeline(self):
        """Kubeflow ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        kubeflow_pipeline = '''from kfp import dsl, components
from kfp.components import InputPath, OutputPath
import kfp

# ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®šç¾©
def load_data_op():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    return components.create_component_from_func(
        func=load_data,
        base_image='python:3.9',
        packages_to_install=['pandas', 'scikit-learn', 'boto3']
    )

def load_data(
    data_path: str,
    output_data_path: OutputPath('Dataset')
) -> str:
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°"""
    import pandas as pd
    import pickle
    import boto3
    
    # S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    s3 = boto3.client('s3')
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯...
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    with open(output_data_path, 'wb') as f:
        pickle.dump(processed_data, f)
    
    return f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(processed_data)} è¡Œ"

def preprocess_data_op():
    """å‰å‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    return components.create_component_from_func(
        func=preprocess_data,
        base_image='python:3.9',
        packages_to_install=['pandas', 'scikit-learn', 'numpy']
    )

def preprocess_data(
    input_data_path: InputPath('Dataset'),
    output_features_path: OutputPath('Features'),
    output_target_path: OutputPath('Target')
) -> str:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•°"""
    import pandas as pd
    import pickle
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(input_data_path, 'rb') as f:
        data = pickle.load(f)
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    # ... å‰å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ ...
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä¿å­˜
    with open(output_features_path, 'wb') as f:
        pickle.dump(X_processed, f)
    
    with open(output_target_path, 'wb') as f:
        pickle.dump(y_processed, f)
    
    return f"å‰å‡¦ç†å®Œäº†: {X_processed.shape}"

def train_model_op():
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    return components.create_component_from_func(
        func=train_model,
        base_image='python:3.9',
        packages_to_install=['scikit-learn', 'mlflow', 'boto3']
    )

def train_model(
    features_path: InputPath('Features'),
    target_path: InputPath('Target'),
    model_path: OutputPath('Model'),
    hyperparameters: dict,
    mlflow_tracking_uri: str
) -> dict:
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–¢æ•°"""
    import pickle
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import accuracy_score, f1_score
    
    # MLflowè¨­å®š
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    with mlflow.start_run():
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(features_path, 'rb') as f:
            X = pickle.load(f)
        with open(target_path, 'rb') as f:
            y = pickle.load(f)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestClassifier(**hyperparameters)
        model.fit(X, y)
        
        # è©•ä¾¡
        cv_scores = cross_val_score(model, X, y, cv=5)
        accuracy = cv_scores.mean()
        
        # MLflowã«ãƒ­ã‚°
        mlflow.log_params(hyperparameters)
        mlflow.log_metric("cv_accuracy", accuracy)
        mlflow.log_metric("cv_std", cv_scores.std())
        mlflow.sklearn.log_model(model, "model")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        return {
            "accuracy": accuracy,
            "std": cv_scores.std(),
            "model_uri": mlflow.get_artifact_uri("model")
        }

def evaluate_model_op():
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    return components.create_component_from_func(
        func=evaluate_model,
        base_image='python:3.9',
        packages_to_install=['scikit-learn', 'pandas']
    )

def evaluate_model(
    model_path: InputPath('Model'),
    test_features_path: InputPath('Features'),
    test_target_path: InputPath('Target'),
    metrics_path: OutputPath('Metrics')
) -> dict:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–¢æ•°"""
    import pickle
    import json
    from sklearn.metrics import classification_report, confusion_matrix
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    with open(test_features_path, 'rb') as f:
        X_test = pickle.load(f)
    
    with open(test_target_path, 'rb') as f:
        y_test = pickle.load(f)
    
    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(X_test)
    
    metrics = {
        "classification_report": classification_report(y_test, y_pred, output_dict=True),
        "confusion_matrix": confusion_matrix(y_test, y_pred).tolist()
    }
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f)
    
    return {
        "accuracy": metrics["classification_report"]["accuracy"],
        "f1_weighted": metrics["classification_report"]["weighted avg"]["f1-score"]
    }

def deploy_model_op():
    """ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    return components.create_component_from_func(
        func=deploy_model,
        base_image='python:3.9',
        packages_to_install=['kubernetes', 'boto3']
    )

def deploy_model(
    model_path: InputPath('Model'),
    deployment_config: dict,
    kubeconfig_path: str
) -> str:
    """ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤é–¢æ•°"""
    import pickle
    import boto3
    from kubernetes import client, config
    import yaml
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    s3 = boto3.client('s3')
    bucket_name = deployment_config['model_bucket']
    model_key = f"models/{deployment_config['model_name']}/model.pkl"
    
    s3.upload_file(model_path, bucket_name, model_key)
    
    # Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
    config.load_kube_config(config_file=kubeconfig_path)
    
    deployment_yaml = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {deployment_config['model_name']}-serving
  namespace: {deployment_config['namespace']}
spec:
  replicas: {deployment_config['replicas']}
  selector:
    matchLabels:
      app: {deployment_config['model_name']}-serving
  template:
    metadata:
      labels:
        app: {deployment_config['model_name']}-serving
    spec:
      containers:
      - name: model-server
        image: {deployment_config['serving_image']}
        env:
        - name: MODEL_S3_URI
          value: s3://{bucket_name}/{model_key}
        - name: MODEL_NAME
          value: {deployment_config['model_name']}
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
"""
    
    # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé©ç”¨
    apps_v1 = client.AppsV1Api()
    deployment = yaml.safe_load(deployment_yaml)
    
    try:
        apps_v1.create_namespaced_deployment(
            namespace=deployment_config['namespace'],
            body=deployment
        )
        return f"ãƒ¢ãƒ‡ãƒ« {deployment_config['model_name']} ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå®Œäº†ã—ã¾ã—ãŸ"
    except Exception as e:
        return f"ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¨ãƒ©ãƒ¼: {str(e)}"

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
@dsl.pipeline(
    name='ML Training Pipeline',
    description='Complete ML training and deployment pipeline'
)
def ml_pipeline(
    data_path: str = 's3://my-bucket/data/training_data.csv',
    hyperparameters: dict = {'n_estimators': 100, 'max_depth': 10},
    mlflow_tracking_uri: str = 'http://mlflow-server:5000',
    deployment_config: dict = {
        'model_name': 'my-model',
        'namespace': 'production',
        'replicas': 3,
        'model_bucket': 'my-model-bucket',
        'serving_image': 'my-registry/model-server:latest'
    }
):
    """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©"""
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    load_data_task = load_data_op()(
        data_path=data_path
    )
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    preprocess_task = preprocess_data_op()(
        input_data_path=load_data_task.outputs['output_data_path']
    )
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    train_task = train_model_op()(
        features_path=preprocess_task.outputs['output_features_path'],
        target_path=preprocess_task.outputs['output_target_path'],
        hyperparameters=hyperparameters,
        mlflow_tracking_uri=mlflow_tracking_uri
    )
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    evaluate_task = evaluate_model_op()(
        model_path=train_task.outputs['model_path'],
        test_features_path=preprocess_task.outputs['output_features_path'],
        test_target_path=preprocess_task.outputs['output_target_path']
    )
    
    # æ¡ä»¶ä»˜ããƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆç²¾åº¦ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®ã¿ï¼‰
    with dsl.Condition(evaluate_task.outputs['accuracy'] > 0.85):
        deploy_task = deploy_model_op()(
            model_path=train_task.outputs['model_path'],
            deployment_config=deployment_config,
            kubeconfig_path='/tmp/kubeconfig'
        )

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == '__main__':
    import kfp
    
    client = kfp.Client()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    kfp.compiler.Compiler().compile(ml_pipeline, 'ml_pipeline.yaml')
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    experiment = client.create_experiment('ml-experiments')
    run = client.run_pipeline(
        experiment.id,
        'ml-training-run',
        'ml_pipeline.yaml',
        params={
            'hyperparameters': {'n_estimators': 200, 'max_depth': 15},
            'data_path': 's3://my-bucket/data/latest_data.csv'
        }
    )
'''
        
        print("=== Kubeflow ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
        print(kubeflow_pipeline)
        
        return kubeflow_pipeline
    
    def create_mlflow_experiment_tracking(self):
        """MLflowå®Ÿé¨“è¿½è·¡"""
        mlflow_tracking = '''import mlflow
import mlflow.sklearn
import mlflow.pytorch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import joblib
import boto3
from datetime import datetime

class MLflowExperimentTracker:
    """MLflowå®Ÿé¨“è¿½è·¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, tracking_uri, experiment_name):
        mlflow.set_tracking_uri(tracking_uri)
        mlflow.set_experiment(experiment_name)
        self.experiment = mlflow.get_experiment_by_name(experiment_name)
    
    def log_data_info(self, data):
        """ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’ãƒ­ã‚°"""
        mlflow.log_param("data_shape", data.shape)
        mlflow.log_param("data_columns", list(data.columns))
        mlflow.log_param("missing_values", data.isnull().sum().sum())
        
        # ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±
        for col in data.select_dtypes(include=[np.number]).columns:
            mlflow.log_metric(f"data_{col}_mean", data[col].mean())
            mlflow.log_metric(f"data_{col}_std", data[col].std())
    
    def log_preprocessing_steps(self, steps):
        """å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ­ã‚°"""
        mlflow.log_param("preprocessing_steps", steps)
    
    def train_and_log_model(self, X_train, X_test, y_train, y_test, model_params):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨çµæœãƒ­ã‚°"""
        with mlflow.start_run() as run:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ­ã‚°
            mlflow.log_params(model_params)
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model = RandomForestClassifier(**model_params, random_state=42)
            model.fit(X_train, y_train)
            
            # äºˆæ¸¬
            train_predictions = model.predict(X_train)
            test_predictions = model.predict(X_test)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã¨ãƒ­ã‚°
            train_metrics = self._calculate_metrics(y_train, train_predictions, "train")
            test_metrics = self._calculate_metrics(y_test, test_predictions, "test")
            
            for metric_name, value in {**train_metrics, **test_metrics}.items():
                mlflow.log_metric(metric_name, value)
            
            # ç‰¹å¾´é‡é‡è¦åº¦
            feature_importance = dict(zip(
                [f"feature_{i}" for i in range(len(model.feature_importances_))],
                model.feature_importances_
            ))
            for feature, importance in feature_importance.items():
                mlflow.log_metric(f"importance_{feature}", importance)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            mlflow.sklearn.log_model(
                model,
                "model",
                registered_model_name="RandomForestClassifier"
            )
            
            # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜
            self._save_artifacts(model, test_predictions, y_test)
            
            # ãƒ¢ãƒ‡ãƒ«ã®ç½²åã¨ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã‚’è¨˜éŒ²
            from mlflow.models.signature import infer_signature
            signature = infer_signature(X_train, train_predictions)
            mlflow.sklearn.log_model(
                model,
                "model_with_signature",
                signature=signature,
                input_example=X_train.iloc[:5]
            )
            
            return run.info.run_id, test_metrics["test_accuracy"]
    
    def _calculate_metrics(self, y_true, y_pred, prefix):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        return {
            f"{prefix}_accuracy": accuracy_score(y_true, y_pred),
            f"{prefix}_f1": f1_score(y_true, y_pred, average='weighted'),
            f"{prefix}_precision": precision_score(y_true, y_pred, average='weighted'),
            f"{prefix}_recall": recall_score(y_true, y_pred, average='weighted')
        }
    
    def _save_artifacts(self, model, predictions, y_true):
        """ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜"""
        import matplotlib.pyplot as plt
        from sklearn.metrics import confusion_matrix, classification_report
        import seaborn as sns
        
        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_true, predictions)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.savefig('confusion_matrix.png')
        mlflow.log_artifact('confusion_matrix.png')
        plt.close()
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        report = classification_report(y_true, predictions, output_dict=True)
        with open('classification_report.json', 'w') as f:
            import json
            json.dump(report, f, indent=2)
        mlflow.log_artifact('classification_report.json')
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
        joblib.dump(model, 'model.pkl')
        mlflow.log_artifact('model.pkl')
    
    def compare_models(self, metric_name="test_accuracy", top_n=5):
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        runs = mlflow.search_runs(
            experiment_ids=[self.experiment.experiment_id],
            order_by=[f"metrics.{metric_name} DESC"],
            max_results=top_n
        )
        
        comparison_df = runs[[
            'run_id', 'start_time', f'metrics.{metric_name}',
            'params.n_estimators', 'params.max_depth'
        ]].copy()
        
        return comparison_df
    
    def promote_best_model(self, metric_name="test_accuracy"):
        """æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"""
        best_run = mlflow.search_runs(
            experiment_ids=[self.experiment.experiment_id],
            order_by=[f"metrics.{metric_name} DESC"],
            max_results=1
        ).iloc[0]
        
        model_uri = f"runs:/{best_run.run_id}/model"
        
        # ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç™»éŒ²
        model_version = mlflow.register_model(
            model_uri=model_uri,
            name="ProductionModel"
        )
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®š
        mlflow.transition_model_version_stage(
            name="ProductionModel",
            version=model_version.version,
            stage="Production"
        )
        
        return model_version

# ä½¿ç”¨ä¾‹
def run_ml_experiment():
    """MLå®Ÿé¨“å®Ÿè¡Œä¾‹"""
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
    from sklearn.datasets import make_classification
    
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=2,
        random_state=42
    )
    
    data = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(X.shape[1])])
    data['target'] = y
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # å®Ÿé¨“è¿½è·¡é–‹å§‹
    tracker = MLflowExperimentTracker(
        tracking_uri="http://localhost:5000",
        experiment_name="RandomForest_Optimization"
    )
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
    param_grid = [
        {'n_estimators': 50, 'max_depth': 5},
        {'n_estimators': 100, 'max_depth': 10},
        {'n_estimators': 200, 'max_depth': 15},
        {'n_estimators': 300, 'max_depth': 20}
    ]
    
    # è¤‡æ•°å®Ÿé¨“å®Ÿè¡Œ
    for params in param_grid:
        run_id, accuracy = tracker.train_and_log_model(
            X_train, X_test, y_train, y_test, params
        )
        print(f"Run {run_id}: Accuracy = {accuracy:.4f}")
    
    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    comparison = tracker.compare_models()
    print("\\nTop 5 Models:")
    print(comparison)
    
    # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã«æ˜‡æ ¼
    best_model = tracker.promote_best_model()
    print(f"\\nBest model promoted to Production: Version {best_model.version}")

if __name__ == "__main__":
    run_ml_experiment()
'''
        
        print("=== MLflowå®Ÿé¨“è¿½è·¡ ===")
        print(mlflow_tracking)
        
        return mlflow_tracking

# ä½¿ç”¨ä¾‹
mlops_pipeline = MLOpsPipeline()
lifecycle = mlops_pipeline.mlops_lifecycle()
kubeflow_pipeline = mlops_pipeline.create_kubeflow_pipeline()
mlflow_tracking = mlops_pipeline.create_mlflow_experiment_tracking()
```

## ğŸ“š ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã“ã®ç‰¹å…¸ã§ã¯ã€AIé§†å‹•é–‹ç™ºã«å¿…è¦ãªDevOpsãƒ»MLOpsã‚¹ã‚­ãƒ«ã‚’åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸã€‚

### ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«
âœ… DevOpsæ–‡åŒ–ã¨åŸå‰‡ã®ç†è§£  
âœ… CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆGitHub Actions, GitLab CI, Jenkinsï¼‰  
âœ… GitOpsæˆ¦ç•¥ã¨ãƒ„ãƒ¼ãƒ«æ´»ç”¨ï¼ˆArgoCD, Fluxï¼‰  
âœ… Infrastructure as Codeå®Ÿè·µï¼ˆTerraform, Pulumiï¼‰  
âœ… MLOpsãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†  
âœ… æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–ï¼ˆKubeflowï¼‰  
âœ… å®Ÿé¨“è¿½è·¡ã¨ãƒ¢ãƒ‡ãƒ«ç®¡ç†ï¼ˆMLflowï¼‰  
âœ… ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°æˆ¦ç•¥  

### å®Ÿè·µçš„ãªå­¦ç¿’èª²é¡Œ
1. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: ãƒ‡ãƒ¼ã‚¿åé›†ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã®å®Œå…¨è‡ªå‹•åŒ–
2. **ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰CI/CD**: è¤‡æ•°ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
3. **ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãƒ‡ãƒ—ãƒ­ã‚¤**: ã‚«ãƒŠãƒªã‚¢ãƒ»ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè£…
4. **ç½å®³å¾©æ—§è‡ªå‹•åŒ–**: è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
5. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£çµ±åˆ**: DevSecOpsãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å®Ÿè£…

### æ¨å¥¨å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹
- **DevOps**: The DevOps Handbook, The Phoenix Project
- **GitOps**: GitOps Cookbook, Argo CD in Action
- **MLOps**: Building Machine Learning Pipelines, MLOps Engineering
- **å®Ÿè·µ**: Kubernetes Operators, Cloud Native DevOps
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: CNCF, DevOps Days, MLOps Community

DevOpsãƒ»MLOpsã¯ç¶™ç¶šçš„ãªæ”¹å–„ãŒéµã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€çµ„ç¹”ã®æ–‡åŒ–å¤‰é©ã¨æŠ€è¡“å®Ÿè£…ã‚’ä¸¡è¼ªã§é€²ã‚ã¦ã„ãã¾ã—ã‚‡ã†ï¼