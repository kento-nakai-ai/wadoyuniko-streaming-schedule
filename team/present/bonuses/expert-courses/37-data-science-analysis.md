# ç‰¹å…¸37: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»åˆ†æã‚¹ã‚­ãƒ«å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰

## ğŸ¯ æ¦‚è¦
AIé§†å‹•é–‹ç™ºã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»åˆ†æã‚¹ã‚­ãƒ«ã‚’ç¶²ç¾…çš„ã«ç¿’å¾—ã™ã‚‹ãŸã‚ã®å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸåˆ†ææ‰‹æ³•ã‹ã‚‰ã€æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®æ´»ç”¨ã¾ã§ã€ç¾å ´ã§å³æˆ¦åŠ›ã¨ãªã‚‹ã‚¹ã‚­ãƒ«ã‚’ä½“ç³»çš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚

## ğŸ“‹ å­¦ç¿’ç›®æ¨™
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®åŸºç¤æ¦‚å¿µã¨å·¥ç¨‹ã‚’ç†è§£ã™ã‚‹
- [ ] pandas/NumPyã‚’ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- [ ] çµ±è¨ˆåˆ†æã¨æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ã‚’å®Ÿè·µã™ã‚‹
- [ ] ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹
- [ ] æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆã¨çµ±è¨ˆçš„æ¤œå®šã‚’å®Ÿè£…ã™ã‚‹
- [ ] æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹

## ğŸš€ Phase 1: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åŸºç¤

### 1.1 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æ¦‚å¿µã¨å·¥ç¨‹

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class DataScienceWorkflow:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¨™æº–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…
    """
    
    def __init__(self, data_path=None):
        self.data_path = data_path
        self.raw_data = None
        self.processed_data = None
        self.model = None
        self.results = {}
    
    def load_data(self, data=None):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        if data is not None:
            self.raw_data = data
        elif self.data_path:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¾‹
            self.raw_data = pd.read_csv(self.data_path)
        else:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
            self.raw_data = self._generate_sample_data()
        
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.raw_data.shape}")
        return self.raw_data
    
    def _generate_sample_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        np.random.seed(42)
        n_samples = 1000
        
        data = {
            'age': np.random.normal(35, 10, n_samples),
            'income': np.random.normal(50000, 15000, n_samples),
            'education_years': np.random.poisson(16, n_samples),
            'experience': np.random.exponential(5, n_samples),
            'satisfaction_score': np.random.beta(2, 5, n_samples) * 10
        }
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç”Ÿæˆï¼ˆè³¼å…¥ç¢ºç‡ï¼‰
        purchase_prob = (
            0.1 * data['age'] / 100 +
            0.3 * data['income'] / 100000 +
            0.2 * data['education_years'] / 20 +
            0.2 * data['experience'] / 10 +
            0.2 * data['satisfaction_score'] / 10
        )
        
        data['purchased'] = np.random.binomial(1, purchase_prob, n_samples)
        
        return pd.DataFrame(data)
    
    def explore_data(self):
        """æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰"""
        print("=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
        print(self.raw_data.info())
        print("\n=== çµ±è¨ˆã‚µãƒãƒªãƒ¼ ===")
        print(self.raw_data.describe())
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        print("\n=== æ¬ æå€¤ ===")
        missing_data = self.raw_data.isnull().sum()
        print(missing_data[missing_data > 0])
        
        # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        plt.figure(figsize=(10, 8))
        correlation_matrix = self.raw_data.corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('å¤‰æ•°é–“ã®ç›¸é–¢é–¢ä¿‚')
        plt.tight_layout()
        plt.show()
        
        return correlation_matrix
    
    def preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        processed = self.raw_data.copy()
        
        # æ¬ æå€¤å‡¦ç†
        for column in processed.select_dtypes(include=[np.number]).columns:
            processed[column].fillna(processed[column].median(), inplace=True)
        
        for column in processed.select_dtypes(include=[object]).columns:
            processed[column].fillna(processed[column].mode()[0], inplace=True)
        
        # å¤–ã‚Œå€¤å‡¦ç†ï¼ˆIQRæ³•ï¼‰
        for column in processed.select_dtypes(include=[np.number]).columns:
            if column != 'purchased':  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã¯é™¤å¤–
                Q1 = processed[column].quantile(0.25)
                Q3 = processed[column].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                processed[column] = processed[column].clip(lower_bound, upper_bound)
        
        self.processed_data = processed
        print("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
        return self.processed_data

# ä½¿ç”¨ä¾‹
workflow = DataScienceWorkflow()
data = workflow.load_data()
correlation_matrix = workflow.explore_data()
processed_data = workflow.preprocess_data()
```

### 1.2 pandasæ´»ç”¨è¡“

```python
class PandasMasterClass:
    """
    pandasã®é«˜åº¦ãªæ´»ç”¨æŠ€è¡“ã‚’ç¿’å¾—
    """
    
    @staticmethod
    def advanced_data_manipulation():
        """é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿æ“ä½œæŠ€è¡“"""
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        dates = pd.date_range('2024-01-01', periods=365, freq='D')
        
        sales_data = pd.DataFrame({
            'date': np.repeat(dates, 3),
            'product': np.tile(['A', 'B', 'C'], 365),
            'sales': np.random.poisson(100, 365 * 3),
            'region': np.random.choice(['North', 'South', 'East', 'West'], 365 * 3),
            'price': np.random.uniform(10, 100, 365 * 3)
        })
        
        print("=== åŸºæœ¬æ“ä½œ ===")
        # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¨é›†è¨ˆ
        monthly_sales = sales_data.groupby([
            sales_data['date'].dt.to_period('M'), 'product'
        ])['sales'].agg(['sum', 'mean', 'std']).round(2)
        
        print("æœˆæ¬¡å£²ä¸Šã‚µãƒãƒªãƒ¼:")
        print(monthly_sales.head(10))
        
        # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        print("\n=== ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ« ===")
        pivot_table = sales_data.pivot_table(
            values='sales',
            index='product',
            columns='region',
            aggfunc=['sum', 'mean'],
            fill_value=0
        )
        print(pivot_table)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°
        print("\n=== ç§»å‹•å¹³å‡ ===")
        sales_data['sales_ma7'] = sales_data.groupby('product')['sales'].rolling(7).mean()
        sales_data['sales_cumsum'] = sales_data.groupby('product')['sales'].cumsum()
        
        # æ¡ä»¶åˆ†å²ã«ã‚ˆã‚‹æ–°ã‚«ãƒ©ãƒ ä½œæˆ
        sales_data['performance'] = np.select(
            [
                sales_data['sales'] > sales_data['sales'].quantile(0.8),
                sales_data['sales'] > sales_data['sales'].quantile(0.6),
                sales_data['sales'] > sales_data['sales'].quantile(0.4),
                sales_data['sales'] > sales_data['sales'].quantile(0.2)
            ],
            ['Excellent', 'Good', 'Average', 'Below Average'],
            default='Poor'
        )
        
        return sales_data
    
    @staticmethod
    def time_series_operations():
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ“ä½œ"""
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        dates = pd.date_range('2024-01-01', periods=365, freq='D')
        ts_data = pd.DataFrame({
            'date': dates,
            'value': np.random.randn(365).cumsum() + 100,
            'seasonal': 10 * np.sin(2 * np.pi * np.arange(365) / 365.25),
            'trend': np.arange(365) * 0.1
        })
        ts_data['total'] = ts_data['value'] + ts_data['seasonal'] + ts_data['trend']
        ts_data.set_index('date', inplace=True)
        
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        weekly_data = ts_data.resample('W').agg({
            'total': ['mean', 'max', 'min'],
            'value': 'sum'
        })
        
        # ãƒ©ã‚°ç‰¹å¾´é‡
        ts_data['lag_1'] = ts_data['total'].shift(1)
        ts_data['lag_7'] = ts_data['total'].shift(7)
        ts_data['lag_30'] = ts_data['total'].shift(30)
        
        # å·®åˆ†
        ts_data['diff_1'] = ts_data['total'].diff()
        ts_data['pct_change'] = ts_data['total'].pct_change()
        
        return ts_data, weekly_data

# å®Ÿè¡Œä¾‹
pandas_master = PandasMasterClass()
sales_data = pandas_master.advanced_data_manipulation()
ts_data, weekly_data = pandas_master.time_series_operations()
```

## ğŸ”¬ Phase 2: çµ±è¨ˆåˆ†æã¨æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ

### 2.1 çµ±è¨ˆçš„æ¤œå®šã¨A/Bãƒ†ã‚¹ãƒˆ

```python
import scipy.stats as stats
from scipy.stats import chi2_contingency, mannwhitneyu, kruskal
import statsmodels.api as sm
from statsmodels.stats.power import ttest_power
from statsmodels.stats.proportion import proportions_ztest

class StatisticalAnalysis:
    """
    çµ±è¨ˆåˆ†æã¨A/Bãƒ†ã‚¹ãƒˆã®å®Ÿè£…
    """
    
    def __init__(self):
        self.results = {}
    
    def ab_test_design(self, effect_size=0.1, alpha=0.05, power=0.8):
        """A/Bãƒ†ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨­è¨ˆ"""
        # å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
        sample_size = ttest_power(effect_size, power, alpha, alternative='two-sided')
        
        print(f"=== A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ ===")
        print(f"åŠ¹æœã‚µã‚¤ã‚º: {effect_size}")
        print(f"æœ‰æ„æ°´æº– (Î±): {alpha}")
        print(f"æ¤œå‡ºåŠ› (1-Î²): {power}")
        print(f"å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {int(sample_size * 2)} (å„ç¾¤)")
        
        return int(sample_size * 2)
    
    def generate_ab_test_data(self, n_per_group=1000, effect_size=0.1):
        """A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        np.random.seed(42)
        
        # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ç¾¤
        control_conversion = np.random.binomial(1, 0.1, n_per_group)
        
        # ãƒ†ã‚¹ãƒˆç¾¤ï¼ˆåŠ¹æœã‚µã‚¤ã‚ºåˆ†ã ã‘æ”¹å–„ï¼‰
        test_conversion = np.random.binomial(1, 0.1 + effect_size, n_per_group)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        data = pd.DataFrame({
            'group': ['control'] * n_per_group + ['test'] * n_per_group,
            'converted': np.concatenate([control_conversion, test_conversion]),
            'revenue': np.concatenate([
                np.random.exponential(50, n_per_group) * control_conversion,
                np.random.exponential(55, n_per_group) * test_conversion
            ])
        })
        
        return data
    
    def analyze_ab_test(self, data):
        """A/Bãƒ†ã‚¹ãƒˆçµæœã®åˆ†æ"""
        # åŸºæœ¬çµ±è¨ˆ
        summary = data.groupby('group').agg({
            'converted': ['count', 'sum', 'mean'],
            'revenue': ['mean', 'sum']
        }).round(4)
        
        print("=== A/Bãƒ†ã‚¹ãƒˆçµæœ ===")
        print(summary)
        
        # ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã®æ¯”è¼ƒï¼ˆZæ¤œå®šï¼‰
        control_conversions = data[data['group'] == 'control']['converted'].sum()
        test_conversions = data[data['group'] == 'test']['converted'].sum()
        control_n = len(data[data['group'] == 'control'])
        test_n = len(data[data['group'] == 'test'])
        
        conversions = np.array([control_conversions, test_conversions])
        nobs = np.array([control_n, test_n])
        
        z_stat, p_value = proportions_ztest(conversions, nobs)
        
        print(f"\n=== ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡æ¯”è¼ƒ ===")
        print(f"Zçµ±è¨ˆé‡: {z_stat:.4f}")
        print(f"på€¤: {p_value:.4f}")
        print(f"çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if p_value < 0.05 else 'éæœ‰æ„'}")
        
        # å£²ä¸Šã®æ¯”è¼ƒï¼ˆMann-Whitney Uæ¤œå®šï¼‰
        control_revenue = data[data['group'] == 'control']['revenue']
        test_revenue = data[data['group'] == 'test']['revenue']
        
        u_stat, u_p_value = mannwhitneyu(
            control_revenue, test_revenue, alternative='two-sided'
        )
        
        print(f"\n=== å£²ä¸Šæ¯”è¼ƒ ===")
        print(f"Mann-Whitney Uçµ±è¨ˆé‡: {u_stat:.4f}")
        print(f"på€¤: {u_p_value:.4f}")
        print(f"çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if u_p_value < 0.05 else 'éæœ‰æ„'}")
        
        return {
            'conversion_z_stat': z_stat,
            'conversion_p_value': p_value,
            'revenue_u_stat': u_stat,
            'revenue_p_value': u_p_value
        }
    
    def cohort_analysis(self, data):
        """ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æã®å®Ÿè£…"""
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ï¼‰
        np.random.seed(42)
        n_users = 1000
        
        # ç™»éŒ²æ—¥ã®ç”Ÿæˆ
        start_date = pd.Timestamp('2024-01-01')
        registration_dates = pd.date_range(
            start_date, periods=90, freq='D'
        )
        
        cohort_data = []
        for reg_date in registration_dates:
            daily_users = np.random.poisson(10)
            for _ in range(daily_users):
                user_id = len(cohort_data)
                
                # ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç¢ºç‡ï¼ˆæ—¥æ•°ãŒçµŒã¤ã»ã©ä½ä¸‹ï¼‰
                retention_probs = np.exp(-np.arange(30) * 0.1)
                
                for day in range(30):
                    if np.random.random() < retention_probs[day]:
                        cohort_data.append({
                            'user_id': user_id,
                            'registration_date': reg_date,
                            'activity_date': reg_date + pd.Timedelta(days=day),
                            'day_number': day
                        })
        
        cohort_df = pd.DataFrame(cohort_data)
        
        # ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
        cohort_table = cohort_df.pivot_table(
            index='registration_date',
            columns='day_number',
            values='user_id',
            aggfunc='nunique'
        )
        
        # ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ã®è¨ˆç®—
        cohort_sizes = cohort_df.groupby('registration_date')['user_id'].nunique()
        retention_table = cohort_table.divide(cohort_sizes, axis=0)
        
        print("=== ã‚³ãƒ›ãƒ¼ãƒˆãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ ===")
        print(retention_table.iloc[:10, :10].round(3))
        
        return retention_table

# ä½¿ç”¨ä¾‹
stat_analyzer = StatisticalAnalysis()
sample_size = stat_analyzer.ab_test_design(effect_size=0.05)
ab_data = stat_analyzer.generate_ab_test_data(n_per_group=sample_size//2)
ab_results = stat_analyzer.analyze_ab_test(ab_data)
retention_table = stat_analyzer.cohort_analysis(ab_data)
```

### 2.2 é«˜åº¦ãªå¯è¦–åŒ–æŠ€è¡“

```python
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

class AdvancedVisualization:
    """
    é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–æŠ€è¡“
    """
    
    def __init__(self):
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
    
    def create_dashboard_chart(self, data):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å½¢å¼ã®ç·åˆãƒãƒ£ãƒ¼ãƒˆ"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡æ¨ç§»', 'å£²ä¸Šåˆ†å¸ƒ',
                'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'ãƒ•ã‚¡ãƒãƒ«åˆ†æ'
            ),
            specs=[[{"secondary_y": True}, {"type": "box"}],
                   [{"type": "bar"}, {"type": "funnel"}]]
        )
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ï¼‰
        dates = pd.date_range('2024-01-01', periods=30, freq='D')
        conversion_rates = np.random.beta(2, 18, 30)
        
        fig.add_trace(
            go.Scatter(
                x=dates, y=conversion_rates,
                mode='lines+markers',
                name='ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡',
                line=dict(color='blue', width=2)
            ),
            row=1, col=1
        )
        
        # å£²ä¸Šåˆ†å¸ƒï¼ˆãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼‰
        segments = ['æ–°è¦', 'ãƒªãƒ”ãƒ¼ãƒˆ', 'VIP']
        revenue_data = [
            np.random.lognormal(3, 0.5, 1000),
            np.random.lognormal(3.5, 0.4, 800),
            np.random.lognormal(4, 0.3, 200)
        ]
        
        for i, (segment, revenue) in enumerate(zip(segments, revenue_data)):
            fig.add_trace(
                go.Box(y=revenue, name=segment),
                row=1, col=2
            )
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        fig.add_trace(
            go.Bar(
                x=segments,
                y=[250000, 180000, 420000],
                name='ç·å£²ä¸Š',
                marker_color=['lightblue', 'lightgreen', 'lightcoral']
            ),
            row=2, col=1
        )
        
        # ãƒ•ã‚¡ãƒãƒ«åˆ†æ
        stages = ['è¨ªå•', 'ã‚«ãƒ¼ãƒˆè¿½åŠ ', 'æ±ºæ¸ˆé–‹å§‹', 'è³¼å…¥å®Œäº†']
        values = [10000, 3000, 1200, 800]
        
        fig.add_trace(
            go.Funnel(
                y=stages,
                x=values,
                textinfo="value+percent initial"
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            height=800,
            title_text="ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
            showlegend=False
        )
        
        return fig
    
    def correlation_network(self, correlation_matrix):
        """ç›¸é–¢é–¢ä¿‚ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³"""
        # å¼·ã„ç›¸é–¢ã®ãƒšã‚¢ã‚’æŠ½å‡º
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.3:  # é–¾å€¤
                    strong_correlations.append({
                        'source': correlation_matrix.columns[i],
                        'target': correlation_matrix.columns[j],
                        'weight': abs(corr_value),
                        'correlation': corr_value
                    })
        
        # NetworkXã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆï¼ˆæ¦‚å¿µçš„ãªä¾‹ï¼‰
        print("=== å¼·ã„ç›¸é–¢é–¢ä¿‚ ===")
        for corr in strong_correlations:
            print(f"{corr['source']} â†” {corr['target']}: {corr['correlation']:.3f}")
        
        return strong_correlations
    
    def anomaly_detection_plot(self, data):
        """ç•°å¸¸æ¤œçŸ¥ã®å¯è¦–åŒ–"""
        from sklearn.ensemble import IsolationForest
        from sklearn.preprocessing import StandardScaler
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        numeric_data = data.select_dtypes(include=[np.number])
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_data)
        
        # ç•°å¸¸æ¤œçŸ¥
        isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = isolation_forest.fit_predict(scaled_data)
        
        # å¯è¦–åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        anomaly_scores = isolation_forest.decision_function(scaled_data)
        axes[0, 0].hist(anomaly_scores, bins=50, alpha=0.7)
        axes[0, 0].axvline(np.percentile(anomaly_scores, 10), color='red', linestyle='--')
        axes[0, 0].set_title('ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('ç•°å¸¸ã‚¹ã‚³ã‚¢')
        axes[0, 0].set_ylabel('é »åº¦')
        
        # 2æ¬¡å…ƒã§ã®ç•°å¸¸ç‚¹è¡¨ç¤º
        if len(numeric_data.columns) >= 2:
            normal_mask = anomaly_labels == 1
            axes[0, 1].scatter(
                numeric_data.iloc[normal_mask, 0],
                numeric_data.iloc[normal_mask, 1],
                c='blue', alpha=0.6, label='æ­£å¸¸'
            )
            axes[0, 1].scatter(
                numeric_data.iloc[~normal_mask, 0],
                numeric_data.iloc[~normal_mask, 1],
                c='red', alpha=0.8, label='ç•°å¸¸'
            )
            axes[0, 1].set_xlabel(numeric_data.columns[0])
            axes[0, 1].set_ylabel(numeric_data.columns[1])
            axes[0, 1].set_title('ç•°å¸¸ç‚¹æ¤œå‡ºçµæœ')
            axes[0, 1].legend()
        
        # æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥ï¼ˆæ™‚é–“è»¸ãŒã‚ã‚‹å ´åˆï¼‰
        if 'date' in data.columns or data.index.dtype == 'datetime64[ns]':
            time_index = data.index if data.index.dtype == 'datetime64[ns]' else range(len(data))
            axes[1, 0].plot(time_index, anomaly_scores, alpha=0.7)
            axes[1, 0].scatter(
                np.array(time_index)[~normal_mask],
                anomaly_scores[~normal_mask],
                c='red', s=50, alpha=0.8
            )
            axes[1, 0].set_title('æ™‚ç³»åˆ—ç•°å¸¸æ¤œçŸ¥')
            axes[1, 0].set_xlabel('æ™‚é–“')
            axes[1, 0].set_ylabel('ç•°å¸¸ã‚¹ã‚³ã‚¢')
        
        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆç•°å¸¸æ¤œçŸ¥ã¸ã®å¯„ä¸ï¼‰
        feature_importance = np.abs(isolation_forest.score_samples(scaled_data)).mean()
        axes[1, 1].bar(numeric_data.columns, np.random.random(len(numeric_data.columns)))
        axes[1, 1].set_title('ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆç•°å¸¸æ¤œçŸ¥ï¼‰')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        return anomaly_labels, anomaly_scores

# ä½¿ç”¨ä¾‹
viz = AdvancedVisualization()
dashboard_fig = viz.create_dashboard_chart(sales_data)
# dashboard_fig.show()  # Plotlyãƒãƒ£ãƒ¼ãƒˆã®è¡¨ç¤º

correlation_network = viz.correlation_network(correlation_matrix)
anomaly_labels, anomaly_scores = viz.anomaly_detection_plot(processed_data)
```

## ğŸ“Š Phase 3: æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 3.1 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

class FeatureEngineering:
    """
    é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æŠ€è¡“
    """
    
    def __init__(self):
        self.feature_transformers = {}
        self.selected_features = []
    
    def create_polynomial_features(self, X, degree=2):
        """å¤šé …å¼ç‰¹å¾´é‡ã®ç”Ÿæˆ"""
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        X_poly = poly.fit_transform(X)
        feature_names = poly.get_feature_names_out(X.columns)
        
        print(f"å¤šé …å¼ç‰¹å¾´é‡ç”Ÿæˆ: {X.shape[1]} â†’ {X_poly.shape[1]}")
        
        self.feature_transformers['polynomial'] = poly
        return pd.DataFrame(X_poly, columns=feature_names, index=X.index)
    
    def create_interaction_features(self, X):
        """äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã®ä½œæˆ"""
        interaction_features = X.copy()
        
        # æ•°å€¤ã‚«ãƒ©ãƒ ã®çµ„ã¿åˆã‚ã›
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                # ä¹—ç®—
                interaction_features[f'{col1}_x_{col2}'] = X[col1] * X[col2]
                # é™¤ç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
                interaction_features[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-8)
                # å·®åˆ†
                interaction_features[f'{col1}_diff_{col2}'] = X[col1] - X[col2]
        
        print(f"äº¤äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆ: {X.shape[1]} â†’ {interaction_features.shape[1]}")
        return interaction_features
    
    def create_binning_features(self, X, n_bins=5):
        """ãƒ“ãƒ‹ãƒ³ã‚°ï¼ˆã‚«ãƒ†ã‚´ãƒªåŒ–ï¼‰ç‰¹å¾´é‡"""
        binned_features = X.copy()
        
        for col in X.select_dtypes(include=[np.number]).columns:
            # ç­‰é »åº¦ãƒ“ãƒ‹ãƒ³ã‚°
            binned_features[f'{col}_binned'] = pd.qcut(
                X[col], q=n_bins, labels=False, duplicates='drop'
            )
            
            # ç­‰å¹…ãƒ“ãƒ‹ãƒ³ã‚°
            binned_features[f'{col}_uniform_binned'] = pd.cut(
                X[col], bins=n_bins, labels=False
            )
        
        return binned_features
    
    def feature_selection(self, X, y, method='mutual_info', k=10):
        """ç‰¹å¾´é‡é¸æŠ"""
        if method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
        elif method == 'f_classif':
            selector = SelectKBest(score_func=f_classif, k=k)
        else:
            raise ValueError("method must be 'mutual_info' or 'f_classif'")
        
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()].tolist()
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
        scores = selector.scores_
        feature_scores = pd.DataFrame({
            'feature': X.columns,
            'score': scores
        }).sort_values('score', ascending=False)
        
        plt.figure(figsize=(10, 6))
        sns.barplot(data=feature_scores.head(k), x='score', y='feature')
        plt.title(f'ç‰¹å¾´é‡é‡è¦åº¦ ({method})')
        plt.tight_layout()
        plt.show()
        
        self.selected_features = selected_features
        self.feature_transformers['selector'] = selector
        
        print(f"ç‰¹å¾´é‡é¸æŠ: {X.shape[1]} â†’ {len(selected_features)}")
        return pd.DataFrame(X_selected, columns=selected_features, index=X.index)
    
    def dimensionality_reduction(self, X, method='pca', n_components=2):
        """æ¬¡å…ƒå‰Šæ¸›"""
        if method == 'pca':
            reducer = PCA(n_components=n_components)
            X_reduced = reducer.fit_transform(X)
            
            # å¯„ä¸ç‡ã®è¡¨ç¤º
            explained_variance = reducer.explained_variance_ratio_
            print(f"PCAå¯„ä¸ç‡: {explained_variance}")
            
        elif method == 'tsne':
            reducer = TSNE(n_components=n_components, random_state=42)
            X_reduced = reducer.fit_transform(X)
        
        self.feature_transformers['dimensionality_reducer'] = reducer
        
        # å¯è¦–åŒ–
        if n_components == 2:
            plt.figure(figsize=(10, 6))
            plt.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.6)
            plt.title(f'{method.upper()} - 2æ¬¡å…ƒå¯è¦–åŒ–')
            plt.xlabel(f'{method.upper()}1')
            plt.ylabel(f'{method.upper()}2')
            plt.show()
        
        return X_reduced

# ä½¿ç”¨ä¾‹
feature_eng = FeatureEngineering()

# æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚’æŠ½å‡º
numeric_features = processed_data.select_dtypes(include=[np.number]).drop('purchased', axis=1)
target = processed_data['purchased']

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
poly_features = feature_eng.create_polynomial_features(numeric_features, degree=2)
interaction_features = feature_eng.create_interaction_features(numeric_features)
binned_features = feature_eng.create_binning_features(numeric_features)

# ç‰¹å¾´é‡é¸æŠ
selected_features = feature_eng.feature_selection(
    interaction_features, target, method='mutual_info', k=15
)

# æ¬¡å…ƒå‰Šæ¸›
reduced_features = feature_eng.dimensionality_reduction(
    selected_features, method='pca', n_components=5
)
```

### 3.2 ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```python
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    classification_report, confusion_matrix
)
import joblib

class ModelEvaluation:
    """
    æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æœ€é©åŒ–
    """
    
    def __init__(self):
        self.models = {}
        self.results = {}
        self.best_model = None
    
    def setup_models(self):
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.models = {
            'logistic_regression': LogisticRegression(random_state=42),
            'random_forest': RandomForestClassifier(random_state=42),
            'gradient_boosting': GradientBoostingClassifier(random_state=42),
            'svm': SVC(probability=True, random_state=42)
        }
        
    def cross_validation_comparison(self, X, y, cv=5):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        cv_results = {}
        
        for name, model in self.models.items():
            scores = cross_val_score(
                model, X, y, cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),
                scoring='roc_auc'
            )
            cv_results[name] = {
                'mean_score': scores.mean(),
                'std_score': scores.std(),
                'scores': scores
            }
            
            print(f"{name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
        
        # çµæœã®å¯è¦–åŒ–
        plt.figure(figsize=(12, 6))
        
        # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        plt.subplot(1, 2, 1)
        scores_data = [cv_results[name]['scores'] for name in cv_results.keys()]
        plt.boxplot(scores_data, labels=cv_results.keys())
        plt.title('ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ')
        plt.ylabel('ROC AUC Score')
        plt.xticks(rotation=45)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
        plt.subplot(1, 2, 2)
        means = [cv_results[name]['mean_score'] for name in cv_results.keys()]
        stds = [cv_results[name]['std_score'] for name in cv_results.keys()]
        plt.bar(cv_results.keys(), means, yerr=stds, capsize=5)
        plt.title('å¹³å‡ã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
        plt.ylabel('ROC AUC Score')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        return cv_results
    
    def hyperparameter_tuning(self, X, y, model_name='random_forest'):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        model = self.models[model_name]
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã®å®šç¾©
        if model_name == 'random_forest':
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [None, 5, 10, 15],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
        elif model_name == 'gradient_boosting':
            param_grid = {
                'n_estimators': [100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5]
            }
        elif model_name == 'logistic_regression':
            param_grid = {
                'C': [0.01, 0.1, 1, 10, 100],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            }
        else:
            param_grid = {}
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ
        grid_search = GridSearchCV(
            model, param_grid, cv=5, scoring='roc_auc',
            n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X, y)
        
        print(f"=== {model_name} ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ ===")
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")
        print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
        
        # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®å¯è¦–åŒ–
        if len(param_grid) >= 2:
            param_names = list(param_grid.keys())[:2]
            results_df = pd.DataFrame(grid_search.cv_results_)
            
            pivot_table = results_df.pivot_table(
                values='mean_test_score',
                index=f'param_{param_names[0]}',
                columns=f'param_{param_names[1]}',
                aggfunc='mean'
            )
            
            plt.figure(figsize=(10, 6))
            sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.4f')
            plt.title(f'{model_name} - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–çµæœ')
            plt.show()
        
        self.best_model = grid_search.best_estimator_
        return grid_search.best_estimator_, grid_search.best_score_
    
    def detailed_evaluation(self, model, X_train, X_test, y_train, y_test):
        """è©³ç´°ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        # äºˆæ¸¬
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        print("=== åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ ===")
        print(classification_report(y_test, y_pred))
        
        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··åŒè¡Œåˆ—')
        plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')
        plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')
        
        # ROCæ›²ç·š
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        plt.subplot(1, 3, 2)
        plt.plot(fpr, tpr, label=f'ROCæ›²ç·š (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], 'k--', label='ãƒ©ãƒ³ãƒ€ãƒ ')
        plt.xlabel('å½é™½æ€§ç‡')
        plt.ylabel('çœŸé™½æ€§ç‡')
        plt.title('ROCæ›²ç·š')
        plt.legend()
        
        # Precision-Recallæ›²ç·š
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        
        plt.subplot(1, 3, 3)
        plt.plot(recall, precision, label='PRæ›²ç·š')
        plt.xlabel('å†ç¾ç‡')
        plt.ylabel('é©åˆç‡')
        plt.title('Precision-Recallæ›²ç·š')
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            plt.figure(figsize=(10, 6))
            sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
            plt.title('ç‰¹å¾´é‡é‡è¦åº¦')
            plt.tight_layout()
            plt.show()
            
            return feature_importance
        
        return None
    
    def save_model(self, model, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        joblib.dump(model, filepath)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        model = joblib.load(filepath)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
        return model

# ä½¿ç”¨ä¾‹
model_eval = ModelEvaluation()
model_eval.setup_models()

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    selected_features, target, test_size=0.2, random_state=42, stratify=target
)

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
cv_results = model_eval.cross_validation_comparison(X_train, y_train)

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
best_rf, best_score = model_eval.hyperparameter_tuning(
    X_train, y_train, 'random_forest'
)

# è©³ç´°è©•ä¾¡
feature_importance = model_eval.detailed_evaluation(
    best_rf, X_train, X_test, y_train, y_test
)

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model_eval.save_model(best_rf, 'best_model.pkl')
```

## ğŸ”® Phase 4: æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬

### 4.1 æ™‚ç³»åˆ—åˆ†æã®å®Ÿè£…

```python
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller

class TimeSeriesAnalysis:
    """
    æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    """
    
    def __init__(self):
        self.ts_data = None
        self.models = {}
        self.forecasts = {}
    
    def generate_sample_timeseries(self, periods=365):
        """ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=periods, freq='D')
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†
        trend = np.linspace(100, 150, periods)
        
        # å­£ç¯€æˆåˆ†
        seasonal = 20 * np.sin(2 * np.pi * np.arange(periods) / 365.25)
        
        # ãƒã‚¤ã‚ºæˆåˆ†
        noise = np.random.normal(0, 5, periods)
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        ts_values = trend + seasonal + noise
        
        self.ts_data = pd.Series(ts_values, index=dates, name='value')
        return self.ts_data
    
    def time_series_decomposition(self):
        """æ™‚ç³»åˆ—åˆ†è§£"""
        if self.ts_data is None:
            self.generate_sample_timeseries()
        
        # å­£ç¯€åˆ†è§£
        decomposition = seasonal_decompose(
            self.ts_data, model='additive', period=365
        )
        
        # å¯è¦–åŒ–
        fig, axes = plt.subplots(4, 1, figsize=(15, 12))
        
        decomposition.observed.plot(ax=axes[0], title='å…ƒãƒ‡ãƒ¼ã‚¿')
        decomposition.trend.plot(ax=axes[1], title='ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†')
        decomposition.seasonal.plot(ax=axes[2], title='å­£ç¯€æˆåˆ†')
        decomposition.resid.plot(ax=axes[3], title='æ®‹å·®æˆåˆ†')
        
        plt.tight_layout()
        plt.show()
        
        return decomposition
    
    def stationarity_test(self):
        """å®šå¸¸æ€§ã®æ¤œå®š"""
        if self.ts_data is None:
            self.generate_sample_timeseries()
        
        # Augmented Dickey-Fulleræ¤œå®š
        result = adfuller(self.ts_data.dropna())
        
        print("=== å®šå¸¸æ€§æ¤œå®š (ADFæ¤œå®š) ===")
        print(f"ADFçµ±è¨ˆé‡: {result[0]:.6f}")
        print(f"på€¤: {result[1]:.6f}")
        print(f"è‡¨ç•Œå€¤:")
        for key, value in result[4].items():
            print(f"\t{key}: {value:.3f}")
        
        if result[1] <= 0.05:
            print("çµæœ: æ™‚ç³»åˆ—ã¯å®šå¸¸ã§ã‚ã‚‹ (p < 0.05)")
        else:
            print("çµæœ: æ™‚ç³»åˆ—ã¯éå®šå¸¸ã§ã‚ã‚‹ (p >= 0.05)")
        
        # å·®åˆ†ç³»åˆ—ã§ã®å®šå¸¸æ€§ç¢ºèª
        diff_series = self.ts_data.diff().dropna()
        diff_result = adfuller(diff_series)
        
        print("\n=== 1æ¬¡å·®åˆ†ç³»åˆ—ã®å®šå¸¸æ€§æ¤œå®š ===")
        print(f"ADFçµ±è¨ˆé‡: {diff_result[0]:.6f}")
        print(f"på€¤: {diff_result[1]:.6f}")
        
        return result, diff_result
    
    def acf_pacf_analysis(self):
        """è‡ªå·±ç›¸é–¢ãƒ»åè‡ªå·±ç›¸é–¢åˆ†æ"""
        if self.ts_data is None:
            self.generate_sample_timeseries()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®ACF/PACF
        plot_acf(self.ts_data.dropna(), lags=40, ax=axes[0, 0], title='ACF (å…ƒãƒ‡ãƒ¼ã‚¿)')
        plot_pacf(self.ts_data.dropna(), lags=40, ax=axes[0, 1], title='PACF (å…ƒãƒ‡ãƒ¼ã‚¿)')
        
        # å·®åˆ†ç³»åˆ—ã®ACF/PACF
        diff_series = self.ts_data.diff().dropna()
        plot_acf(diff_series, lags=40, ax=axes[1, 0], title='ACF (1æ¬¡å·®åˆ†)')
        plot_pacf(diff_series, lags=40, ax=axes[1, 1], title='PACF (1æ¬¡å·®åˆ†)')
        
        plt.tight_layout()
        plt.show()
    
    def arima_modeling(self, order=(1, 1, 1)):
        """ARIMAãƒ¢ãƒ‡ãƒªãƒ³ã‚°"""
        if self.ts_data is None:
            self.generate_sample_timeseries()
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_size = int(len(self.ts_data) * 0.8)
        train_data = self.ts_data[:train_size]
        test_data = self.ts_data[train_size:]
        
        # ARIMAãƒ¢ãƒ‡ãƒ«
        arima_model = ARIMA(train_data, order=order)
        arima_fitted = arima_model.fit()
        
        print("=== ARIMAãƒ¢ãƒ‡ãƒ«çµæœ ===")
        print(arima_fitted.summary())
        
        # äºˆæ¸¬
        forecast_steps = len(test_data)
        forecast = arima_fitted.forecast(steps=forecast_steps)
        forecast_ci = arima_fitted.get_forecast(steps=forecast_steps).conf_int()
        
        # å¯è¦–åŒ–
        plt.figure(figsize=(15, 8))
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        plt.plot(train_data.index, train_data.values, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', color='blue')
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        plt.plot(test_data.index, test_data.values, label='å®Ÿæ¸¬å€¤', color='green')
        
        # äºˆæ¸¬å€¤
        plt.plot(test_data.index, forecast, label='äºˆæ¸¬å€¤', color='red')
        
        # ä¿¡é ¼åŒºé–“
        plt.fill_between(test_data.index, 
                        forecast_ci.iloc[:, 0], 
                        forecast_ci.iloc[:, 1], 
                        color='red', alpha=0.3, label='95%ä¿¡é ¼åŒºé–“')
        
        plt.title('ARIMAäºˆæ¸¬çµæœ')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        # äºˆæ¸¬ç²¾åº¦è©•ä¾¡
        mse = np.mean((test_data.values - forecast) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(test_data.values - forecast))
        
        print(f"\n=== äºˆæ¸¬ç²¾åº¦ ===")
        print(f"MSE: {mse:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"MAE: {mae:.4f}")
        
        self.models['arima'] = arima_fitted
        self.forecasts['arima'] = forecast
        
        return arima_fitted, forecast
    
    def exponential_smoothing(self):
        """æŒ‡æ•°å¹³æ»‘æ³•"""
        if self.ts_data is None:
            self.generate_sample_timeseries()
        
        train_size = int(len(self.ts_data) * 0.8)
        train_data = self.ts_data[:train_size]
        test_data = self.ts_data[train_size:]
        
        # Holt-Wintersæ³•
        hw_model = ExponentialSmoothing(
            train_data, 
            trend='add', 
            seasonal='add', 
            seasonal_periods=365
        )
        hw_fitted = hw_model.fit()
        
        # äºˆæ¸¬
        forecast = hw_fitted.forecast(steps=len(test_data))
        
        # å¯è¦–åŒ–
        plt.figure(figsize=(15, 8))
        plt.plot(train_data.index, train_data.values, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿')
        plt.plot(test_data.index, test_data.values, label='å®Ÿæ¸¬å€¤')
        plt.plot(test_data.index, forecast, label='Holt-Wintersäºˆæ¸¬')
        plt.title('Holt-Wintersæ³•ã«ã‚ˆã‚‹äºˆæ¸¬')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        # ç²¾åº¦è©•ä¾¡
        mse = np.mean((test_data.values - forecast) ** 2)
        print(f"Holt-Winters RMSE: {np.sqrt(mse):.4f}")
        
        self.models['holt_winters'] = hw_fitted
        self.forecasts['holt_winters'] = forecast
        
        return hw_fitted, forecast

# ä½¿ç”¨ä¾‹
ts_analyzer = TimeSeriesAnalysis()
ts_data = ts_analyzer.generate_sample_timeseries(periods=730)  # 2å¹´åˆ†
decomposition = ts_analyzer.time_series_decomposition()
stationarity_results = ts_analyzer.stationarity_test()
ts_analyzer.acf_pacf_analysis()
arima_model, arima_forecast = ts_analyzer.arima_modeling(order=(2, 1, 2))
hw_model, hw_forecast = ts_analyzer.exponential_smoothing()
```

## ğŸŒ Phase 5: ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã¨MLOps

### 5.1 ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†

```python
import dask.dataframe as dd
from dask.distributed import Client
import sqlite3

class BigDataProcessing:
    """
    ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãŸã‚ã®å®Ÿè£…
    """
    
    def __init__(self):
        self.client = None
        self.data_chunks = []
    
    def setup_dask_client(self, n_workers=2):
        """Daskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.client = Client(n_workers=n_workers, threads_per_worker=2)
        print(f"Daskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆèµ·å‹•: {self.client.dashboard_link}")
        return self.client
    
    def create_large_dataset(self, n_files=5, rows_per_file=100000):
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ"""
        for i in range(n_files):
            np.random.seed(i)
            data = pd.DataFrame({
                'user_id': np.random.randint(1, 10000, rows_per_file),
                'timestamp': pd.date_range(
                    '2024-01-01', periods=rows_per_file, freq='1min'
                ),
                'event_type': np.random.choice(
                    ['click', 'view', 'purchase'], rows_per_file,
                    p=[0.7, 0.25, 0.05]
                ),
                'value': np.random.exponential(50, rows_per_file),
                'category': np.random.choice(
                    ['A', 'B', 'C', 'D'], rows_per_file
                )
            })
            
            filename = f'large_data_{i}.csv'
            data.to_csv(filename, index=False)
            print(f"ç”Ÿæˆå®Œäº†: {filename}")
    
    def dask_data_processing(self):
        """Daskã‚’ä½¿ã£ãŸä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        # Daskãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã¿
        dask_df = dd.read_csv('large_data_*.csv')
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(dask_df)} è¡Œ (æ¨å®š)")
        print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {dask_df.npartitions}")
        
        # ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é›†è¨ˆ
        results = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        results['basic_stats'] = dask_df.describe().compute()
        
        # ã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆ
        results['event_summary'] = dask_df.groupby('event_type').agg({
            'value': ['sum', 'mean', 'count'],
            'user_id': 'nunique'
        }).compute()
        
        # æ™‚ç³»åˆ—é›†è¨ˆ
        dask_df['hour'] = dd.to_datetime(dask_df['timestamp']).dt.hour
        results['hourly_summary'] = dask_df.groupby(['hour', 'event_type'])['value'].sum().compute()
        
        print("=== ã‚¤ãƒ™ãƒ³ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼ ===")
        print(results['event_summary'])
        
        return results
    
    def streaming_data_simulation(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æ¨¡æ“¬å®Ÿè£…"""
        def process_batch(batch_data):
            """ãƒãƒƒãƒå‡¦ç†é–¢æ•°"""
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ
            batch_summary = batch_data.groupby('event_type').agg({
                'value': ['sum', 'mean', 'count']
            })
            
            # ç•°å¸¸æ¤œçŸ¥
            anomalies = batch_data[
                batch_data['value'] > batch_data['value'].quantile(0.99)
            ]
            
            return {
                'summary': batch_summary,
                'anomalies': anomalies,
                'timestamp': pd.Timestamp.now()
            }
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        batch_size = 1000
        streaming_results = []
        
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒƒãƒã«åˆ†å‰²
        full_data = pd.concat([
            pd.read_csv(f'large_data_{i}.csv') for i in range(2)
        ])
        
        for i in range(0, len(full_data), batch_size):
            batch = full_data.iloc[i:i+batch_size]
            result = process_batch(batch)
            streaming_results.append(result)
            
            if i % (batch_size * 10) == 0:  # 10ãƒãƒƒãƒã”ã¨ã«é€²æ—è¡¨ç¤º
                print(f"å‡¦ç†æ¸ˆã¿ãƒãƒƒãƒ: {i//batch_size + 1}")
        
        return streaming_results

# SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æº
class DatabaseIntegration:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æºã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    """
    
    def __init__(self, db_path='analytics.db'):
        self.db_path = db_path
        self.conn = None
    
    def setup_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.conn = sqlite3.connect(self.db_path)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        create_table_query = '''
        CREATE TABLE IF NOT EXISTS user_events (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            timestamp TEXT,
            event_type TEXT,
            value REAL,
            category TEXT
        )
        '''
        
        self.conn.execute(create_table_query)
        self.conn.commit()
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    
    def bulk_insert_data(self):
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªæŒ¿å…¥"""
        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
        for i in range(2):  # æœ€åˆã®2ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            df = pd.read_csv(f'large_data_{i}.csv')
            df.to_sql('user_events', self.conn, if_exists='append', index=False)
            print(f"æŒ¿å…¥å®Œäº†: large_data_{i}.csv")
    
    def sql_analytics(self):
        """SQLã«ã‚ˆã‚‹åˆ†æã‚¯ã‚¨ãƒª"""
        queries = {
            'daily_summary': '''
                SELECT 
                    DATE(timestamp) as date,
                    event_type,
                    COUNT(*) as event_count,
                    SUM(value) as total_value,
                    AVG(value) as avg_value
                FROM user_events 
                GROUP BY DATE(timestamp), event_type
                ORDER BY date, event_type
            ''',
            
            'user_behavior': '''
                SELECT 
                    user_id,
                    COUNT(DISTINCT event_type) as event_types,
                    COUNT(*) as total_events,
                    SUM(value) as total_value
                FROM user_events 
                GROUP BY user_id
                HAVING total_events >= 10
                ORDER BY total_value DESC
                LIMIT 20
            ''',
            
            'conversion_funnel': '''
                WITH user_events_agg AS (
                    SELECT 
                        user_id,
                        MAX(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) as has_view,
                        MAX(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) as has_click,
                        MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as has_purchase
                    FROM user_events
                    GROUP BY user_id
                )
                SELECT 
                    SUM(has_view) as viewers,
                    SUM(has_click) as clickers,
                    SUM(has_purchase) as purchasers,
                    ROUND(SUM(has_click) * 100.0 / SUM(has_view), 2) as click_rate,
                    ROUND(SUM(has_purchase) * 100.0 / SUM(has_click), 2) as conversion_rate
                FROM user_events_agg
                WHERE has_view = 1
            '''
        }
        
        results = {}
        for name, query in queries.items():
            results[name] = pd.read_sql_query(query, self.conn)
            print(f"=== {name} ===")
            print(results[name])
            print()
        
        return results
    
    def close_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self.conn:
            self.conn.close()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    big_data_processor = BigDataProcessing()
    
    # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
    big_data_processor.create_large_dataset(n_files=3, rows_per_file=50000)
    
    # Daskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆèµ·å‹•
    client = big_data_processor.setup_dask_client()
    
    # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
    processing_results = big_data_processor.dask_data_processing()
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    streaming_results = big_data_processor.streaming_data_simulation()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æº
    db_integration = DatabaseIntegration()
    db_integration.setup_database()
    db_integration.bulk_insert_data()
    sql_results = db_integration.sql_analytics()
    db_integration.close_connection()
    
    print("ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")
```

### 5.2 ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
import pickle
from datetime import datetime
import json

class ModelDeployment:
    """
    æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    """
    
    def __init__(self, model_name="production_model"):
        self.model_name = model_name
        self.model = None
        self.model_metadata = {}
        self.prediction_log = []
        self.performance_metrics = {}
    
    def prepare_model_for_deployment(self, trained_model, feature_names, target_name):
        """ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™"""
        self.model = trained_model
        self.model_metadata = {
            'model_type': type(trained_model).__name__,
            'feature_names': feature_names,
            'target_name': target_name,
            'deployment_date': datetime.now().isoformat(),
            'version': '1.0.0'
        }
        
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        model_package = {
            'model': trained_model,
            'metadata': self.model_metadata
        }
        
        with open(f'{self.model_name}.pkl', 'wb') as f:
            pickle.dump(model_package, f)
        
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæº–å‚™å®Œäº†: {self.model_name}")
        return model_package
    
    def load_production_model(self, model_path=None):
        """æœ¬ç•ªç’°å¢ƒãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        if model_path is None:
            model_path = f'{self.model_name}.pkl'
        
        with open(model_path, 'rb') as f:
            model_package = pickle.load(f)
        
        self.model = model_package['model']
        self.model_metadata = model_package['metadata']
        
        print(f"æœ¬ç•ªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.model_metadata['version']}")
        return self.model
    
    def predict_with_logging(self, input_data, user_id=None):
        """äºˆæ¸¬ã¨ãƒ­ã‚°è¨˜éŒ²"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # äºˆæ¸¬å®Ÿè¡Œ
        prediction_start = datetime.now()
        
        if hasattr(input_data, 'values'):
            prediction = self.model.predict(input_data.values.reshape(1, -1))[0]
            prediction_proba = None
            if hasattr(self.model, 'predict_proba'):
                prediction_proba = self.model.predict_proba(input_data.values.reshape(1, -1))[0]
        else:
            prediction = self.model.predict([input_data])[0]
            prediction_proba = None
            if hasattr(self.model, 'predict_proba'):
                prediction_proba = self.model.predict_proba([input_data])[0]
        
        prediction_end = datetime.now()
        
        # ãƒ­ã‚°è¨˜éŒ²
        log_entry = {
            'timestamp': prediction_start.isoformat(),
            'user_id': user_id,
            'input_data': input_data.tolist() if hasattr(input_data, 'tolist') else input_data,
            'prediction': float(prediction),
            'prediction_proba': prediction_proba.tolist() if prediction_proba is not None else None,
            'latency_ms': (prediction_end - prediction_start).total_seconds() * 1000,
            'model_version': self.model_metadata['version']
        }
        
        self.prediction_log.append(log_entry)
        
        return prediction, prediction_proba
    
    def batch_prediction(self, batch_data):
        """ãƒãƒƒãƒäºˆæ¸¬å‡¦ç†"""
        batch_start = datetime.now()
        
        predictions = self.model.predict(batch_data)
        prediction_probas = None
        if hasattr(self.model, 'predict_proba'):
            prediction_probas = self.model.predict_proba(batch_data)
        
        batch_end = datetime.now()
        
        # ãƒãƒƒãƒãƒ­ã‚°
        batch_log = {
            'timestamp': batch_start.isoformat(),
            'batch_size': len(batch_data),
            'total_latency_ms': (batch_end - batch_start).total_seconds() * 1000,
            'avg_latency_per_sample': (batch_end - batch_start).total_seconds() * 1000 / len(batch_data),
            'model_version': self.model_metadata['version']
        }
        
        self.prediction_log.append(batch_log)
        
        return predictions, prediction_probas
    
    def monitor_model_performance(self, true_labels=None):
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
        if not self.prediction_log:
            print("äºˆæ¸¬ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†æ
        latencies = [log.get('latency_ms', 0) for log in self.prediction_log if 'latency_ms' in log]
        
        if latencies:
            latency_stats = {
                'mean_latency': np.mean(latencies),
                'median_latency': np.median(latencies),
                'p95_latency': np.percentile(latencies, 95),
                'p99_latency': np.percentile(latencies, 99)
            }
            
            print("=== ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·çµ±è¨ˆ ===")
            for metric, value in latency_stats.items():
                print(f"{metric}: {value:.2f} ms")
        
        # äºˆæ¸¬åˆ†å¸ƒåˆ†æ
        predictions = [log.get('prediction') for log in self.prediction_log if 'prediction' in log]
        
        if predictions:
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.hist(predictions, bins=50, alpha=0.7)
            plt.title('äºˆæ¸¬å€¤ã®åˆ†å¸ƒ')
            plt.xlabel('äºˆæ¸¬å€¤')
            plt.ylabel('é »åº¦')
            
            plt.subplot(1, 2, 2)
            if latencies:
                plt.hist(latencies, bins=50, alpha=0.7)
                plt.title('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®åˆ†å¸ƒ')
                plt.xlabel('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms)')
                plt.ylabel('é »åº¦')
            
            plt.tight_layout()
            plt.show()
        
        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºï¼ˆæ¦‚å¿µå®Ÿè£…ï¼‰
        self.detect_data_drift()
        
        return {
            'latency_stats': latency_stats if latencies else {},
            'prediction_stats': {
                'count': len(predictions),
                'mean': np.mean(predictions) if predictions else 0,
                'std': np.std(predictions) if predictions else 0
            }
        }
    
    def detect_data_drift(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®æ¤œå‡º"""
        if len(self.prediction_log) < 100:
            print("ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã«ã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
            return
        
        # æœ€è¿‘ã®äºˆæ¸¬ã¨éå»ã®äºˆæ¸¬ã‚’æ¯”è¼ƒ
        recent_predictions = [log.get('prediction') for log in self.prediction_log[-50:] if 'prediction' in log]
        historical_predictions = [log.get('prediction') for log in self.prediction_log[:-50] if 'prediction' in log]
        
        if recent_predictions and historical_predictions:
            # KSæ¤œå®šã«ã‚ˆã‚‹åˆ†å¸ƒã®æ¯”è¼ƒ
            from scipy.stats import ks_2samp
            
            ks_stat, p_value = ks_2samp(historical_predictions, recent_predictions)
            
            print("=== ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º ===")
            print(f"KSçµ±è¨ˆé‡: {ks_stat:.4f}")
            print(f"på€¤: {p_value:.4f}")
            
            if p_value < 0.05:
                print("âš ï¸ æœ‰æ„ãªãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            else:
                print("âœ… æœ‰æ„ãªãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            return {'ks_stat': ks_stat, 'p_value': p_value}
    
    def generate_monitoring_report(self):
        """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            'model_info': self.model_metadata,
            'prediction_count': len(self.prediction_log),
            'monitoring_period': {
                'start': self.prediction_log[0]['timestamp'] if self.prediction_log else None,
                'end': self.prediction_log[-1]['timestamp'] if self.prediction_log else None
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§ä¿å­˜
        report_filename = f'{self.model_name}_monitoring_report.json'
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_filename}")
        return report

# ä½¿ç”¨ä¾‹
deployment = ModelDeployment("vibe_coding_model")

# å‰å›ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ã«æº–å‚™
model_package = deployment.prepare_model_for_deployment(
    best_rf, 
    selected_features.columns.tolist(),
    'purchased'
)

# æœ¬ç•ªç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
production_model = deployment.load_production_model()

# äºˆæ¸¬ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
for i in range(100):
    # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    sample_input = selected_features.iloc[i]
    prediction, prediction_proba = deployment.predict_with_logging(
        sample_input, user_id=f"user_{i}"
    )

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
monitoring_results = deployment.monitor_model_performance()

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
monitoring_report = deployment.generate_monitoring_report()
```

## ğŸ¯ å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ãƒãƒªãƒ¥ãƒ¼ï¼ˆCLVï¼‰äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

```python
class CLVPredictionSystem:
    """
    é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
    """
    
    def __init__(self):
        self.data_pipeline = None
        self.feature_engineer = None
        self.model_pipeline = None
        self.deployment_system = None
    
    def setup_complete_pipeline(self):
        """å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.data_pipeline = DataScienceWorkflow()
        
        # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        self.feature_engineer = FeatureEngineering()
        
        # 3. ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.model_pipeline = ModelEvaluation()
        self.model_pipeline.setup_models()
        
        # 4. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
        self.deployment_system = ModelDeployment("clv_model")
        
        print("CLVäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    
    def generate_clv_data(self, n_customers=5000):
        """CLVäºˆæ¸¬ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        np.random.seed(42)
        
        # é¡§å®¢åŸºæœ¬æƒ…å ±
        customers = pd.DataFrame({
            'customer_id': range(1, n_customers + 1),
            'age': np.random.normal(40, 15, n_customers),
            'income': np.random.lognormal(10, 0.5, n_customers),
            'education_years': np.random.poisson(16, n_customers),
            'city_tier': np.random.choice([1, 2, 3], n_customers, p=[0.3, 0.5, 0.2]),
            'acquisition_channel': np.random.choice(['online', 'offline', 'referral'], n_customers, p=[0.5, 0.3, 0.2])
        })
        
        # è¡Œå‹•ãƒ‡ãƒ¼ã‚¿
        customers['days_since_first_purchase'] = np.random.exponential(200, n_customers)
        customers['total_orders'] = np.random.poisson(
            5 + customers['income'] / 50000 + customers['age'] / 20, n_customers
        )
        customers['avg_order_value'] = np.random.lognormal(
            4 + customers['income'] / 100000, 0.5, n_customers
        )
        customers['recency_days'] = np.random.exponential(30, n_customers)
        
        # CLVè¨ˆç®—ï¼ˆçœŸã®å€¤ï¼‰
        customers['clv'] = (
            customers['avg_order_value'] * 
            customers['total_orders'] * 
            (365 / customers['recency_days'].clip(1, 365)) *
            np.random.uniform(0.8, 1.2, n_customers)
        )
        
        # é«˜ä¾¡å€¤é¡§å®¢ãƒ•ãƒ©ã‚°ï¼ˆäºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
        customers['high_value'] = (customers['clv'] > customers['clv'].quantile(0.7)).astype(int)
        
        return customers
    
    def end_to_end_pipeline(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        print("=== CLVäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===")
        
        # 1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å‰å‡¦ç†
        print("1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å‰å‡¦ç†...")
        clv_data = self.generate_clv_data()
        self.data_pipeline.raw_data = clv_data
        processed_data = self.data_pipeline.preprocess_data()
        
        # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print("2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...")
        feature_cols = ['age', 'income', 'education_years', 'days_since_first_purchase', 
                       'total_orders', 'avg_order_value', 'recency_days']
        X = processed_data[feature_cols]
        y = processed_data['high_value']
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        X_encoded = pd.get_dummies(X)
        
        # äº¤äº’ä½œç”¨ç‰¹å¾´é‡
        interaction_features = self.feature_engineer.create_interaction_features(X_encoded)
        
        # ç‰¹å¾´é‡é¸æŠ
        selected_features = self.feature_engineer.feature_selection(
            interaction_features, y, method='mutual_info', k=20
        )
        
        # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡
        print("3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡...")
        X_train, X_test, y_train, y_test = train_test_split(
            selected_features, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        cv_results = self.model_pipeline.cross_validation_comparison(X_train, y_train)
        
        # æœ€é©åŒ–
        best_model, best_score = self.model_pipeline.hyperparameter_tuning(
            X_train, y_train, 'random_forest'
        )
        
        # è©³ç´°è©•ä¾¡
        feature_importance = self.model_pipeline.detailed_evaluation(
            best_model, X_train, X_test, y_train, y_test
        )
        
        # 4. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
        print("4. ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ...")
        model_package = self.deployment_system.prepare_model_for_deployment(
            best_model, selected_features.columns.tolist(), 'high_value'
        )
        
        # 5. A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ
        print("5. A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ...")
        stat_analyzer = StatisticalAnalysis()
        ab_sample_size = stat_analyzer.ab_test_design(effect_size=0.1)
        
        print("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        
        return {
            'processed_data': processed_data,
            'best_model': best_model,
            'feature_importance': feature_importance,
            'cv_results': cv_results,
            'deployment_package': model_package
        }

# ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
clv_system = CLVPredictionSystem()
clv_system.setup_complete_pipeline()
pipeline_results = clv_system.end_to_end_pipeline()
```

## ğŸ“š ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã“ã®ç‰¹å…¸ã§ã¯ã€AIé§†å‹•é–‹ç™ºã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»åˆ†æã‚¹ã‚­ãƒ«ã‚’åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸã€‚

### ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«
âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åŸºç¤æ¦‚å¿µã¨å®Ÿè£…  
âœ… pandas/NumPyã«ã‚ˆã‚‹é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿æ“ä½œ  
âœ… çµ±è¨ˆåˆ†æã¨A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ  
âœ… é«˜åº¦ãªå¯è¦–åŒ–æŠ€è¡“  
âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°  
âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨æœ€é©åŒ–  
âœ… æ™‚ç³»åˆ—åˆ†æã¨äºˆæ¸¬  
âœ… ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†æŠ€è¡“  
âœ… MLOpsã¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ  
âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰  

### å®Ÿè·µçš„ãªå­¦ç¿’èª²é¡Œ
1. **ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®åˆ†æ**: æ¥­ç•Œç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã§åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
2. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ **: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º
3. **A/Bãƒ†ã‚¹ãƒˆå®Ÿè£…**: å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã§ã®A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆã¨åˆ†æ
4. **æ™‚ç³»åˆ—äºˆæ¸¬**: ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
5. **MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: CI/CDã‚’å«ã‚€å®Œå…¨ãªMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

### æ¨å¥¨å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹
- **æ›¸ç±**: "Hands-On Machine Learning", "Python for Data Analysis"
- **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹**: Coursera Machine Learning, edX Data Science
- **å®Ÿè·µãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: Kaggle, Google Colab
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: PyData, MLOps Community

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¯ç¶™ç¶šçš„ãªå­¦ç¿’ã¨å®Ÿè·µãŒé‡è¦ã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§çµŒé¨“ã‚’ç©ã‚“ã§ã„ãã¾ã—ã‚‡ã†ï¼